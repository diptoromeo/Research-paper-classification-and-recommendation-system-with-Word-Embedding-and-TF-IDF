<paper no=1>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>BCFL logging: An approach to acquire and preserve admissible digital forensics evidence in cloud ecosystem</paper_heading>
	<author>Kenny Awuson-David, Tawfik Al-Hadhrami, Mamoun Alazab, Nazaraf Shah, Andrii Shalaginov</author>
	<abstract>Log files are the primary source of recording users, applications and protocols, activities in the cloud ecosystem. Cloud forensic investigators can use log evidence to ascertain when, why and how a cyber adversary or an insider compromised a system by establishing the crime scene and reconstructing how the incident occurred. However, digital evidence acquisition in a cloud ecosystem is complicated and proven difficult, even with modern forensic acquisition toolkit. The multi-tenancy, Geo-location and Service-Level Agreement have added another layer of complexity in acquiring digital log evidence from a cloud ecosystem. In order to mitigate these complexities of evidence acquisition in the cloud ecosystem, we need a framework that can forensically maintain the trustworthiness and integrity of log evidence. In this paper, we design and implement a Blockchain Cloud Forensic Logging (BCFL) framework, using a Design Science Research Methodological (DSRM) approach. BCFL operates primarily in four stages: (1) Process transaction logs using Blockchain distributed ledger technology (DLT). (2) Use a Blockchain smart contract to maintain the integrity of logs and establish a clear chain of custody. (3) Validate all transaction logs. (4) Maintain transaction log immutability. BCFL will also enhance and strengthen compliance with the European Union (EU) General Data Protection Regulation (GDPR). The results from our single case study will demonstrate that BCFL will mitigate the challenges and complexities faced by digital forensics investigators in acquiring admissible digital evidence from the cloud ecosystem. Furthermore, an instantaneous performance monitoring of the proposed Blockchain cloud forensic logging framework was evaluated. BCFL will ensure trustworthiness, integrity, authenticity and non-repudiation of the log evidence in the cloud.</abstract>
	<keyword>Blockchain;DSRM;GDPR;Digital log evidence;Trustworthiness;Admissibility</keyword>
	<publication_year>2021_vol_122</publication_year>
</paper>
<paper no=2>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Leveraging user access patterns and advanced cyberinfrastructure to accelerate data delivery from shared-use scientific observatories</paper_heading>
	<author>Yubo Qin, Ivan Rodero, Anthony Simonet, Charles Meertens, ... Manish Parashar</author>
	<abstract>With the growing number and increasing availability of shared-use instruments and observatories, observational data is becoming an essential part of application workflows and contributor to scientific discoveries in a range of disciplines. However, the corresponding growth in the number of users accessing these facilities coupled with the expansion in the scale and variety of the data, is making it challenging for these facilities to ensure their data can be accessed, integrated, and analyzed in a timely manner, and is resulting significant demands on their cyberinfrastructure (CI). In this paper, we present the design of a push-based data delivery framework that leverages emerging in-network capabilities, along with data pre-fetching techniques based on a hybrid data management model. Specifically, we analyze data access traces for two large-scale observatories, Ocean Observatories Initiative (OOI) and Geodetic Facility for the Advancement of Geoscience (GAGE), to identify typical user access patterns and to develop a model that can be used for data pre-fetching. Furthermore, we evaluate our data pre-fetching model and the proposed framework using a simulation of the Virtual Data Collaboratory (VDC) platform that provides in-network data staging and processing capabilities. The results demonstrate that the ability of the framework to significantly improve data delivery performance and reduce network traffic at the observatories’ facilities.</abstract>
	<keyword>Cyberinfrastructure;Virtual Data Collaboratory;Distributed data sharing;Distributed facilities;Data pre-fetching;Observatories</keyword>
	<publication_year>2021_vol_122</publication_year>
</paper>
<paper no=3>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Tenant-defined service function chaining in a multi-site network slice</paper_heading>
	<author>Federica Paganelli, Paola Cappanera, Giovanni Cuffaro</author>
	<abstract>Service Function Chaining (SFC) enables dynamic and adaptive network service provisioning. However, virtual infrastructure providers are reluctant to disclose SFC APIs and infrastructure monitoring information to tenants, therefore undermining tenants’ capability in dynamically and flexibly managing application-tailored network services. In this work, we provide a two-fold contribution to the problem of tenant-defined service function chaining on a multi-site virtual infrastructure, which has not been widely investigated yet. First, we propose a VNF Selection algorithm that relies on an abstracted network model thus minimizing topological and monitoring information required by the infrastructure. Our algorithm selects a set of VNF instances that minimizes the end-to-end latency (accounting for network and processing delay) and also guarantees that already established chains do not violate their maximum latency constraint due to the additional load at VNFs brought by new chain requests. Second, we describe an SFC solution composed of an SFC application that uses our VNF Selection algorithm and a forwarding mechanism that manages chains and controls traffic steering within the tenant network without leveraging infrastructure control APIs. Validation and comparative evaluations are performed through simulations. Finally, the proposed approach is assessed through a set of experiments carried out on a multi-site testbed infrastructure.</abstract>
	<keyword>Network Function Virtualization;Service Function Chaining;Optimization;Tenant network;Software Defined Networking;Intent interface</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=4>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Deeply feature learning by CMAC network for manipulating rehabilitation robots</paper_heading>
	<author>Xing Li, Zijiang Zhu, Nan Shen, Weihuang Dai, Yi Hu</author>
	<abstract>Human action recognition is a key component in modern artificial intelligent systems that greatly enhance the manipulating of various robots, such as rehabilitation robtos and industrial robots. Existing action recognition algorithms mainly depend on a predefined spatial sequence code book, which may fail to discover discriminative spatial–temporal features to mimic robots. In this paper, we propose to engineer the spatial–temporal action features that can deeply encode the similarity of within-class human actions and dissimilarity of between-class human actions. Specifically, given a series of training action video samples, we first segment each video into multiple key sections based on human contour. These sections of a video are related to time and space. Then, local robot action and appearance information are combined using a cerebellar model articulation controller (CMAC) to represent each video section. We quantize these extracted features into a feature vector, which can represent category-specific robot actions. Subsequently, we develop an improved linear discriminative analysis to project the data points to a subspace, where data points with the same label are close while data points with different labels are far from each other. Experimental results on the well-known HMDB51 and KTH datasets have shown the effectiveness and robustness of our method. Moreover, our action recognition can be applied as a key module in the real-world rehabilitation robots.</abstract>
	<keyword>Robot control;CMAC deep model;Human action recognition;Spatial–temporal feature;LDADeep model</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=5>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Deep spatial–temporal sequence modeling for multi-step passenger demand prediction</paper_heading>
	<author>Lei Bai, Lina Yao, Xianzhi Wang, Can Li, Xiang Zhang</author>
	<abstract>Supply–demand imbalance poses significant challenges to transportation systems such as taxis and shared vehicles (cars and bikes) and leads to excessive delays, income loss, and energy consumption. Accurate prediction of passenger demands is an essential step towards rescheduling resources to resolve the above challenges. However, existing work cannot fully capture and leverage the complex nonlinear spatial–temporal relationships within multi-modal data. They either include excessive data from weakly-correlated regions or oversight the correlations among those similar yet geographically distant regions. Moreover, these methods mainly focus on predicting the passenger demand for one future time step, whereas predictions over longer time scales are more valuable for developing efficient vehicle deployment strategies. We propose an end-to-end deep learning based framework to solve the above challenges. Our model comprises three parts: (1) a cascade graph convolutional recurrent neural network to extract spatial–temporal correlations within citywide historical vehicle demand data; (2) two multi-layer LSTM networks to represent the external meteorological data and time meta separately; (3) an encoder–decoder module to fuse the above two parts and decode the representation to achieve prediction over a longer time period into the future. We evaluate our framework on three real-world datasets and show that our model can better capture the spatial–temporal relationships and outperform the most discriminative state-of-the-art methods.</abstract>
	<keyword>Passenger demand prediction;Spatial–temporal correlations;Graph convolutional network;Long-short term memory</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=6>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DGT: A contribution-aware differential gradient transmission mechanism for distributed machine learning</paper_heading>
	<author>Huaman Zhou, Zonghang Li, Qingqing Cai, Hongfang Yu, ... Gang Sun</author>
	<abstract>Distributed machine learning is a mainstream system to learn insights for analytics and intelligence services of many fronts (e.g., health, streaming and business) from their massive operational data. In such a system, multiple workers train over subsets of data and collaboratively derive a global prediction/inference model by iteratively synchronizing their local learning results, e.g., the model gradients, which in turn generates heavy and bursty traffic and results in high communication overhead in cluster networks. Such communication overhead has became the main bottleneck that limits the efficiency of training machine learning models distributedly. In this paper, our key observation is that local gradients learned by workers may have different contributions to global model convergence and executing differential transmission for different gradients can reduce the communication overhead and improve training efficiency. However, existing gradient transmission mechanisms treat all gradients the same, which may lead to long training time. Motivated by our observations, we propose Differential Gradient Transmission (DGT), a contribution-aware differential gradient transmission mechanism for efficient distributed learning, which transfers gradients with different transmission quality according to their contributions. In addition to designing a general architecture of DGT, we have proposed a novel algorithm and a novel protocol to facilitate fast model training. Experiments on a cluster with 6 GTX 1080TI GPUs and 1Gbps network show that DGT decreases the model training time by 19.4% on GoogleNet, 34.4% on AlexNet and 36.5% on VGG-11 compared to default gradient transmission on MXNET. Its acceleration is better than the other two related transmission solutions. Besides, DGT works well with different datasets (Fashion-MNIST, Cifar10), different data distributions (IID, non-IID) and different training algorithms (BSP, FedAVG).</abstract>
	<keyword>Distributed Machine Learning (DML); Gradient transmission; Parameter server architecture</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=7>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Resource allocation and scheduling in the intelligent edge computing context</paper_heading>
	<author>Jun Liu, Tianfu Yang, Jingpan Bai, Bo Sun</author>
	<abstract>With the rapid development of the edge computing, Internet economy has become a new situation in economic development. What has brought development to the Internet economy is a variety of family-based platforms (i.e., edge computing). However, in the face of huge Internet users and different users’ shopping habits, reasonably personalized e-commerce platform allocations for users can improve the user’s website shopping experience. However, due to the huge user data and the diversity of SDN (software defined network) platforms, it is a huge challenge to reasonably allocate e-commerce resources/platforms to users. The quality of the SDN personalized resource allocations also affects the purchase conversion rate. Clustering algorithm is an algorithm involved in grouping data in machine learning. The same set of data has the same attributes and characteristics, and the attributes or features between different sets of data will be relatively large. In this paper, by using the mean shift clustering algorithm to characterize the behavior data. According to the characteristics of the grouping, we can allocate the e-commerce platform commonly used between the groups. However, using mean shift clustering for personalized allocation faces the problem of too high user data dimensions. Therefore, we first conduct computational efficiency analysis toward each user. We define user behavior sequences for user behavior data and classify user behavior. We transform the grouped user behavior into an embedded vector, and linearly transform the embedded vectors of different lengths into the same semantic space. We process the vectors in the semantic space through the self-attention layer and perform mean shift clustering. Experiments show that, in the edge computing context, our method can reduce the complexity of resource allocation toward complex data and improve the quality of the allocated data.</abstract>
	<keyword>Edge computing; Deep learning; Mean shift; Personalized allocation; Scheduling; Software defined network (SDN)</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=8>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Massive picture retrieval system based on big data image mining</paper_heading>
	<author>Kun Zhang, Kai Chen, Binghui Fan</author>
	<abstract>The traditional picture retrieval system has a slow retrieval speed, poor retrieval accuracy, and a low recall when performing massive picture retrieval. In this paper, we design a massive picture retrieval system using the big data image mining technology. It is constructed with data processing layer, business logic layer and presentation layer and works through three steps of data segmentation, mining and merging. For instance, it runs the distributed file system module in a Master/Slave operation mode and designs file read and write requests according to user interaction. Next, it performs parallel computing of picture data sets based on Map Reduce module to solve the picture matching and similarity metrics and returns to the user sorted picture matching result. Then, it extracts the color and texture features of the target area to generate the final picture retrieval result. We select a large number of pictures on a big data platform as simulation test set. The results show that the system we designed has a good retrieval accuracy and a high retrieval speed, which greatly improves the recall of picture retrieval.</abstract>
	<keyword>Big data image;Candidate mode;Parallel computing;Picture retrieval;Distributed file system;Retrieval system</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=9>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Secure data transmission in IoTs based on CLoG edge detection</paper_heading>
	<author>Aiman Jan, Shabir A. Parah, Bilal A. Malik, Mamoon Rashid</author>
	<abstract>Cyber–physical Systems (CPS) have revolutionized the industry by utilizing the Internet of Things (IoT) for interconnecting various system components. IoT plays a significant role in the development of the smart system paradigm by building a link between physical components with the virtual world for improving smart services and quality of life. It significantly improves people’s activities, be it their personalized health care, living, or the way they monitor control, and organize the businesses. However, data streaming/communication over an open IoT environment creates several security issues that need to be taken care of, for transferring data securely. This paper presents an efficient information embedding solution for ensuring data security in a cyber–physicalnetwork. In this work, a new efficient edge detector called CLoG, (based on Canny and Laplacian of Gaussian detectors) has been developed and is used for the detection of edge areas in digital images. The secret information has been embedded in detected edges. The proposed detector finds finer edge details compared to state-of-art, making it possible to hide more information in a cover image. This, in turn, reduces the number of cover images required to transmit secret data and as such fulfills the requirements of resource-constrained platforms like IoT. Experimental results show that the proposed scheme is capable of providing high-quality stego-images with an average peak signal-to-noise ratio (PSNR) of 48.12 dB for a payload of 2 bits per pixel (bpp) payload. Further, the scheme has been analyzed for its efficacy in terms of histogram analysis, normalized cross-correlation (NCC), and other objective parameters. Besides, we show that the proposed scheme supports blind extraction, has less computational complexity, and is suitable for IoT based systems.</abstract>
	<keyword>Cyber–Physical Systems;Security;Information embedding;Edge detection;IoT</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=10>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards enabling I/O awareness in task-based programming models</paper_heading>
	<author>Hatem Elshazly, Jorge Ejarque, Francesc Lordan, Rosa M. Badia</author>
	<abstract>Storage systems have not kept the same technology improvement rate as computing systems. As applications produce more and more data, I/O becomes the limiting factor for increasing application performance. I/O congestion caused by concurrent access to storage devices is one of the main obstacles that cause I/O performance degradation and, consequently, total performance degradation. Although task-based programming models made it possible to achieve higher levels of parallelism by enabling the execution of tasks in large-scale distributed platforms, this parallelism only benefited the compute workload of the application. Previous efforts addressing I/O performance bottlenecks either focused on optimizing fine-grained I/O access patterns using I/O libraries or avoiding system-wide I/O congestion by minimizing interference between multiple applications. In this paper, we propose enabling I/O Awareness in task-based programming models for improving the total performance of applications. An I/O aware programming model is able to create more parallelism and mitigate the causes of I/O performance degradation. On the one hand, more parallelism can be created by supporting special tasks for executing I/O workloads, called I/O tasks, that can overlap with the execution of compute tasks. On the other hand, I/O congestion can be mitigated by constraining I/O tasks scheduling. We propose two approaches for specifying such constraints: explicitly set by the users or automatically inferred and tuned during application’s execution to optimize the execution of variable I/O workloads on a certain storage infrastructure. We implement our proposal using PyCOMPSs: a Task-based programming model for parallelizing Python applications. Our experiments on the MareNostrum 4 Supercomputer demonstrate that using I/O aware PyCOMPSs can achieve significant performance improvement in the total execution time of applications with different I/O workloads. This performance improvement can reach up to 43% of total application performance as compared to the I/O non-aware version of PyCOMPSs.</abstract>
	<keyword>I/O awareness;Task-based programming models;I/O intensive application;sI/O congestion;I/O-compute overlap;I/O scheduling;Auto-tunable constraints</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=11>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A two-stage Multi-Criteria Optimization method for service placement in decentralized edge micro-clouds</paper_heading>
	<author>Javier Panadero, Mennan Selimi, Laura Calvet, Joan Manuel Marquès, Felix Freitag</author>
	<abstract>Community networks are becoming increasingly popular due to the growing demand for network connectivity in both rural and urban areas. Community networks are owned and managed at the edge by volunteers. Their irregular topology, the heterogeneity of resources and their unreliable behavior claim for advanced optimization methods to place services in the network. In particular, an efficient service placement method is key for the performance of these systems. This work presents the Multi-Criteria Optimal Placement method, a novel and fast two-stage multi-objective method to place services in decentralized community network edge micro-clouds. A comprehensive set of computational experiments is carried out using real traces of , which is the largest production community network worldwide. According to the results, the proposed method outperforms both the random placement method used currently in  and the Bandwidth-aware Service Placement method, which provides the best known solutions in the literature, by a mean gap in bandwidth gain of about 53% and 10%, respectively, while it also reduces the number of resources used.</abstract>
	<keyword>Service placement;Distributed systems;Community networks;Micro-clouds;Multi-objective optimization algorithms</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=12>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An improved AlexNet model for automated skeletal maturity assessment using hand X-ray images</paper_heading>
	<author>Ming He, Xudong Zhao, Yu Lu, Yi Hu</author>
	<abstract>There are several problems with traditional artificial skeletal age assessment methods, such as strong subjectivity, large random errors, complex assessment processes, and long assessment cycles. In this study, automated skeletal bone age assessment based on deep learning were performed. Other than examining all the bones in the whole hand, we have proposed a skeletal maturity assessment method based on the Standards of Skeletal Development of Hand and Wrist for Chinese (CHN) method that observing only 14 representative hand bones using the deep convolutional neural network (CNN). The method was compared with the traditional method using whole hand evaluated using the same test dataset. We have also expanded the dataset and increase the generalisation ability of the CNN using data augmentation. As a result, this method is able to improve the accuracy of the final skeletal age assessment and reduce the upper limit of the absolute value of the single skeletal age error. The experiments demonstrate the effectiveness of the proposed method, which can provide physicians with more stable, efficient, and convenient diagnostic assistance and decision support.</abstract>
	<keyword>Skeletal age assessment;Deep learning;Convolutional neural network;Image processing;CHN method</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=13>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A CP-ABE scheme based on multi-authority in hybrid clouds for mobile devices</paper_heading>
	<author>Mande Xie, Yingying Ruan, Haibo Hong, Jun Shao</author>
	<abstract>With the rapid development of cloud computing, the ensuing security issues have become increasingly prominent. In this context, attribute-based encryption (ABE) has gradually attracted widespread attentions due to its unique attribute matching mechanism. At the same time, mobile devices have put forward higher requirements for the design and deployment of ABE schemes. Therefore, in this paper, we propose an original hybrid cloud multi-authority ciphertext-policy attribute-based encryption (HCMACP-ABE) scheme. Specifically, we utilize the LSSS (Linear Secret-Sharing Schemes) access structure to realize secure access control, while the private cloud is responsible for maintaining the user’s authorization list and verifying the user. Finally, we prove that our proposal achieves IND-CCA secure and is efficient in a mobile hybrid cloud environment.</abstract>
	<keyword>CP-ABE;Fine-grained control;Hybrid cloud;Multi-authority</keyword>
	<publication_year>2021_vol_121</publication_year>
</paper>
<paper no=14>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Internet-of-Forensic (IoF): A blockchain based digital forensics framework for IoT applications</paper_heading>
	<author>Gulshan Kumar, Rahul Saha, Chhagan Lal, Mauro Conti</author>
	<abstract>Digital forensic in Internet-of-Thing (IoT) paradigm is critical due to its heterogeneity and lack of transparency of evidence processing. Moreover, cross-border legalization makes a hindrance in such process pertaining to the cloud forensic issues. This urges a forensic framework for IoT which provides distributed computing, decentralization, and transparency of forensic investigation of digital evidences in cross-border perspectives. To this end, we propose a framework for IoT forensics that addresses the above mentioned issues. The proposed solution called Internet-of-Forensics (IoF) considers a blockchain tailored IoT framework for digital forensics. It provides a transparent view of the investigation process that involves all the stakeholders (e.g., heterogeneous devices, and cloud service providers) in a single framework. It uses blockchain-based case chain to deal with the investigation process including chain-of-custody and evidence chain. Consensus is used for consortium to solve the problems of cross-border legalization. This is also beneficial for a transparent and ease of forensic reference. The programmable lattice-based cryptographic primitives produce reduced complexities. It shows benefits for power-aware devices and puts an add-on to the novelty of the presented idea. IoF is generic; hence, it can be used by autonomous security operation centers, cyber-forensic investigators and manually initiated evidences under chain-of-custody for man-made crimes. Security services are assured as required by the framework. IoF is experimented and compared with the other state-of-the-art frameworks. The outcomes and analysis prove the efficiency of IoF concerning complexity, time consumption, memory and CPU utilization, gas consumption, and energy analysis.</abstract>
	<keyword>IoT;Forensics;Digital;Block;chain;Custody;Security;Cloud</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=15>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Research on unsupervised feature learning for Android malware detection based on Restricted Boltzmann Machines</paper_heading>
	<author>Zhen Liu, Ruoyu Wang, Nathalie Japkowicz, Deyu Tang, ... Jie Zhao</author>
	<abstract>Android malware detection has attracted much attention in recent years. Existing methods mainly research on extracting static or dynamic features from mobile apps and build mobile malware detection model by machine learning algorithms. The number of extracted static or dynamic features maybe much high. As a result, the data suffers from high dimensionality. In addition, to avoid being detected, malware data is varied and hard to obtain in the first place. To detect zeroday malware, unsupervised malware detection methods were applied. In such case, unsupervised feature reduction method is an available choice to reduce the data dimensionality. In this paper, we propose an unsupervised feature learning algorithm called Subspace based Restricted Boltzmann Machines (SRBM) for reducing data dimensionality in malware detection. Multiple subspaces in the original data are firstly searched. And then, an RBM is built on each subspace. All outputs of the hidden layers of the trained RBMs are combined to represent the data in lower dimension. The experimental results on OmniDroid, CIC2019 and CIC2020 datasets show that the features learned by SRBM perform better than the ones learned by other feature reduction methods when the performance is evaluated by clustering evaluation metrics, i.e., NMI, ACC and Fscore.</abstract>
	<keyword>Mobile malware detection;Unsupervised feature learning;Restricted Boltzmann Machines;Feature subspaces</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=16>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>FedSA: A staleness-aware asynchronous Federated Learning algorithm with non-IID data</paper_heading>
	<author>Ming Chen, Bingcheng Mao, Tianyi Ma</author>
	<abstract>This paper presents new asynchronous methods to the Federated Learning (FL), one of the next-generation paradigms for Artificial Intelligence (AI) systems. We consider the two-fold challenges lay ahead. First, non-IID (non-Independent and Identically Distributed) data across devices cause unstable performance. Second, unreliable and slow environments not only slow the convergence but also cause staleness issues. To address these challenges, this study uses a bottom-up approach for analysis and algorithm design. We first reformulate FL by unifying both synchronous and asynchronous updating schemes with an asynchrony-related parameter. We theoretically analyze this new form and find practical strategies for optimization. The key findings include: 1) a two-stage training strategy to accelerate training and reduce communication overhead; 2) strategies of choosing key hyperparameters optimally for these stages to maintain efficiency and robustness. With these theoretical guarantees, we propose FedSA (Federated Staleness-Aware), a novel asynchronous federated learning algorithm. We validate FedSA on different tasks with non-IID/IID and staleness settings. Our results indicate that, given a large proportion of stale devices, the proposed algorithm presents state-of-the-art performance by outperforming existing methods on both non-IID and IID cases.</abstract>
	<keyword>Federated Learning;Distributed machine learning;Mobile edge computing;Non-IID data</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=17>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Deeply learning a discriminative spatial–temporal feature for robot action understanding</paper_heading>
	<author>Jun Liu, Bo Sun, Jingpan Bai</author>
	<abstract>Human action recognition is a key component in modern artificial intelligent systems, such as sport analysis, video surveillance and human–computer interaction (HCI). Existing action recognition algorithms mainly depend on a predefined spatial sequence code book, which may fail to discover discriminative spatial–temporal features. In this paper, we propose to engineer the spatial–temporal action features that can deeply encode the similarity of within-class human actions and dissimilarity of between-class human actions. Specifically, given a series of training action video samples, we first segment each video into multiple key sections based on human contour. These sections of a video are related to time and space. Then, local human action and appearance information are combined to represent each video section. We quantize these extracted features into a feature vector, which can represent category-specific human actions. Subsequently, we develop an improved linear discriminative analysis to project the data points to a subspace, where data points with the same label are close while data points with different labels are far from each other. Experimental results on HMDB51 and KTH datasets have shown the effectiveness and robustness of our method. Moreover, the recognized human action sequence can guide the operation of robots in industrial area.</abstract>
	<keyword>Human action recognition;Spatial–temporal feature;LDA;Deep model;Human action sequence</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=18>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Tracking and collision avoidance of virtual coupling train control system</paper_heading>
	<author>Yuan Cao, Jiakun Wen, Lianchuan Ma</author>
	<abstract>In rail transit systems, improving transportation efficiency has become a research hotspot. In recent years, a new method of train control system based on virtual coupling has attracted the attention of many scholars. And the train operation control method is not only the key to realize the virtual coupling train operation control system but also the key to prevent accidents. To this end, a virtual coupling implementation scheme based on local leader–follower method is proposed in this paper. Then, a dynamic model of virtual coupling is established. Based on the dynamics model, the minimum tracking distance and the expected tracking distance which are used to prevent accidents are calculated. The recursive least square method based on the train operation process data is used to identify the model parameters of the virtual coupling train formation operation process. A controller based on generalized model predictive and mixed artificial potential field are used to perform cooperative control and preventing collision of the virtual coupling train. Finally, a section of Beijing–Shanghai high-speed railway is used as the background to verify the validity of the method.</abstract>
	<keyword>Train control system;Virtual coupling;Generalized model predictive control;Dynamic model;Artificial potential field</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=19>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Research on cyberspace multi-objective security algorithm and decision mechanism of Energy Internet</paper_heading>
	<author>Rui Hou, Guowen Ren, Wei Gao, Lijun Liu</author>
	<abstract>The existing energy network security defense system uses firewall, intrusion detection, host monitoring, identity authentication, anti-virus software and vulnerability repair to build a fortress type rigid defense system to block or isolate external invasion. This static layered deep defense system is based on prior knowledge and has the advantages of rapid response and effective protection in the face of constant attacks When confronting the unknown attacking opponent, he is not able to do his best, and he is in danger of being attacked easily. In this context, multi-objective decision has more than two decision-making objectives and needs to use multiple criteria to evaluate and optimize the decision-making of alternatives. Due to the objectives of economic benefit, safety in production and environmental protection, it is necessary to use a variety of criteria to evaluate and optimize schemes. In this paper, we propose RBF neural network and weight-based algorithm to achieve multi-objective decision. We leverage RBF neural network to construct objective weight assignment model. The goal of our weight-based algorithm is that the multi-objective optimization problem is formulated as a single-objective optimization problem by assigning certain weights to each objective, and then the non-inferior solution of the multi-objective optimization problem is generated by changing the weights of each objective extensive.</abstract>
	<keyword>Security algorithm;Energy Internet;Multi-objective decision-making;RBF neural network</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=20>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the impact of knowledge-based linguistic annotations in the quality of scientific embeddings</paper_heading>
	<author>Andres Garcia-Silva, Ronald Denaux, Jose Manuel Gomez-Perez</author>
	<abstract>In essence, embedding algorithms work by optimizing the distance between a word and its usual context in order to generate an embedding space that encodes the distributional representation of words. In addition to single words or word pieces, other features which result from the linguistic analysis of text, including lexical, grammatical and semantic information, can be used to improve the quality of embedding spaces. However, until now we did not have a precise understanding of the impact that such individual annotations and their possible combinations may have in the quality of the embeddings. In this paper, we conduct a comprehensive study on the use of explicit linguistic annotations to generate embeddings from a scientific corpus and quantify their impact in the resulting representations. Our results show how the effect of such annotations in the embeddings varies depending on the evaluation task. In general, we observe that learning embeddings using linguistic annotations contributes to achieve better evaluation results.</abstract>
	<keyword>Natural language processing;Linguistic analysis;Knowledge graphs;Embeddings</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=21>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Extracting knowledge from Deep Neural Networks through graph analysis</paper_heading>
	<author>Vitor A.C. Horta, Ilaria Tiddi, Suzanne Little, Alessandra Mileo</author>
	<abstract>The popularity of deep learning has increased tremendously in recent years due to its ability to efficiently solve complex tasks in challenging areas such as computer vision and language processing. Despite this success, low-level neural activity reproduced by Deep Neural Networks (DNNs) generates extremely rich representations of the data. These representations are difficult to characterise and cannot be directly used to understand the decision process. In this paper we build upon our exploratory work where we introduced the concept of a co-activation graph and investigated the potential of graph analysis for explaining deep representations. The co-activation graph encodes statistical correlations between neurons’ activation values and therefore helps to characterise the relationship between pairs of neurons in the hidden layers and output classes. To confirm the validity of our findings, our experimental evaluation is extended to consider datasets and models with different levels of complexity. For each of the considered datasets we explore the co-activation graph and use graph analysis to detect similar classes, find central nodes and use graph visualisation to better interpret the outcomes of the analysis. Our results show that graph analysis can reveal important insights into how DNNs work and enable partial explainability of deep learning models.</abstract>
	<keyword>Explainable AI;Deep representation learning;Graph analysis</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=22>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Rewarding reviews with tokens: An Ethereum-based approach</paper_heading>
	<author>Andrea Lisi, Andrea De Salve, Paolo Mori, Laura Ricci, Samuel Fabrizi</author>
	<abstract>Recommender Systems (RSs) are becoming increasingly popular in the last years. They collect reviews concerning several types of items (e.g., shops, professionals, services, songs or videos) in order to rank them according to a given criterion, and to suggest the most relevant ones to their users. However, most of the currently used RSs exhibit two main drawbacks: they are based on a centralized control model and they do not provide reward mechanisms to encourage the participation of users. To deal with these challenges, the architectures of current RSs could be enhanced through blockchain technology, thus providing novel solutions to decentralize them. As a matter of fact, the blockchain technology could be successfully adopted in this context because smart contracts would allow the decentralization of system control, while cryptocurrency and tokens could be used to implement the reward mechanism. In the light of the above considerations, this manuscript presents a decentralized rating framework aimed to support the users of RSs based on blockchain technology, providing a token-based reward mechanism that remunerates users submitting their reviews to incentivize their participation. Moreover, the proposed system provides a flexible strategy to rank items, allowing users to choose among different functions to combine reviews to obtain item ranking. The performance and the cost of using the proposed system have been evaluated on the Ropsten Ethereum test network. For instance, our experiments have shown that the median time required to store a batch of 35 ratings is about 47 s, while the average time required to obtain the score of an item having 6000 ratings is less than 2.5 s.</abstract>
	<keyword>Distributed Ledger Technology;Recommender System;Blockchain;Smart contract;Rewarding system</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=23>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Secure crowd-sensing protocol for fog-based vehicular cloud</paper_heading>
	<author>Lewis Nkenyereye, S.M. Riazul Islam, Muhammad Bilal, M. Abdullah-Al-Wadud, ... Anand Nayyar</author>
	<abstract>The new paradigm of fog computing was extended from conventional cloud computing to provide computing and storage capabilities at the edge of the network. Applied to vehicular networks, fog-enabled vehicular computing is expected to become a core feature that can accelerate a multitude of services including crowd-sensing. Accordingly, the security and privacy of vehicles joining the crowd-sensing system have become important issues for cyber defense and smart policing. In addition, to satisfy the demand of crowd-sensing data users, fine-grained access control is required. In this paper, we propose a secure and privacy-preserving crowd-sensing scheme for fog-enabled vehicular computing. The proposed architecture is made by a double layer of fog nodes that is used to generate crowd-sensing tasks for vehicles, then collect, aggregate and analyze the data based on user specifications. To ensure data confidentiality and fined-grained access control, we make use of ciphertext-policy attribute-based encryption with access update policy (CP-ABE-UP), which is a well-known one-to-many encryption technique. The policy update algorithm allows the fog nodes to outsource the crowd-sensing data to other fog nodes or to data users directly. We also adopted the ID-based signature tied to pseudonymous techniques to guarantee the authentication and privacy-preservation of the entities in the system. From the upper fog layer to the data user, we show that an information-centric networking (ICN) approach can be applied to maximize the network resources and enhance the security by avoiding unauthorized and unauthenticated data owners. The security analysis confirms that our approach is secure against known attacks, whereas the simulation results show its efficiency in terms of communication with little computational overhead.</abstract>
	<keyword>Privacy preservation;Crowd-sensing;Fog enabled vehicular computing;Access control;ID-based signature;Attribute-based encryption;Information-centric network</keyword>
	<publication_year>2021_vol_120</publication_year>
</paper>
<paper no=24>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Intrusion and anomaly detection for the next-generation of industrial automation and control systems</paper_heading>
	<author>Luis Rosa, Tiago Cruz, Miguel Borges de Freitas, Pedro Quitério, ... Paulo Simões</author>
	<abstract>The next-generation of Industrial Automation and Control Systems (IACS) and Supervisory Control and Data Acquisition (SCADA) systems pose numerous challenges in terms of cybersecurity monitoring. We have been witnessing the convergence of OT/IT networks, combined with massively distributed metering and control scenarios such as smart grids. Larger and geographically widespread attack surfaces, and inherently more data to analyse, will become the norm. Despite several advances in recent years, domain-specific security tools have been facing the challenges of trying to catch up with all the existing security flaws from the past, while also accounting for the specific needs of the next-generation of IACS. Moreover, the aggregation of multiple techniques and sources of information into a comprehensive approach has not been explored in depth. Such a holistic perspective is paramount since it enables a global and enhanced analysis enabled by the usage, combination and aggregation of the outputs from multiple sources and techniques.This paper starts by providing a review of the more recent anomaly detection techniques for SCADA systems, focused on both theoretical machine learning approaches and complete frameworks. Afterwards, it proposes a complete framework for an Intrusion and Anomaly Detection System (IADS) composed of specific detection probes, an event processing layer and a core anomaly detection component, amongst others. Finally, the paper presents an evaluation of the framework within a large-scale hybrid testbed, and a comparison of different anomaly detection scenarios based on various machine learning techniques.</abstract>
	<keyword>IACS;Industrial control systems;SCADA;Cybersecurity;Critical infrastructure protection;Network anomaly detectionIntrusion detection;Event processing</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=25>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Energy efficiency with service availability guarantee for Network Function Virtualization</paper_heading>
	<author>Liuyang Mai, Yi Ding, Xiaoning Zhang, Lang Fan, ... Zhichao Xu</author>
	<abstract>Following the trend of Network Function Virtualization (NFV), dedicated hardware middleboxes are replaced with innovative and flexible software middleboxes also known as Virtual Network Functions (VNFs). An ordered sequence of VNFs composing a logical service is called Service Function Chain (SFC). VNFs are generally run on commodity servers. In this way, the capital and operational expenditures of buying and maintaining dedicated hardware for telecom operators can be greatly reduced. One of the key issues in NFV is the optimal VNF placement and service chaining to achieve energy efficiency. However, the current NFV energy saving approaches seem to consider energy minimization as the only objective to be optimized. Little or no attention is given to other important aspects, e.g., service availability, which is paramountly important to fulfill Service Level Agreement (SLA) for telecom operators. This paper investigates the energy efficiency optimization with service availability guarantee in NFV-enabled networks. We firstly propose a novel green orchestration NFV architecture. Then, an energy-efficient VNF placement framework guaranteeing service availability is presented under the proposed architecture, and evaluated by extensive simulations. Open research issues and technical challenges in this emerging area are also presented.</abstract>
	<keyword>Network Function Virtualization;Service function chain;Reliability;Protection;Lagrangian relaxation</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=26>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Guaranteeing end-to-end QoS provisioning in SOA based SDN architecture: A survey and Open Issues</paper_heading>
	<author>Shuraia Khan, Farookh Khadeer Hussain, Omar K. Hussain</author>
	<abstract>Ensuring end-to-end Quality of Services (QoS) is a challenging aspect in traditional network architectures. Software-Defined Network (SDN), as the new norm of the network, has ascended in response to a traditional network’s limitations. SDN’s benefits are its ability to provide a global networking view, programmability, decouple the data plane with the control plane. Integrating SDN architecture with Service-Oriented Architecture (SOA) paradigm brings a novel network-based notion for service delivery. However, it also introduces new challenges for maintaining the QoS in these networks. Researchers from both academia and industry have proposed and developed several resolutions for QoS management in SDNs. However, gaps still exist in developing and applying such resolutions for QoS management in SOA-based SDNs. This review paper aims to identify these gaps by representing a sketch of the effectiveness of the existing QoS management techniques in SOA-based SDNs. We first identify the four different requirements that QoS management techniques need to meet to be applied in SOA-based SDNs. We then categorize the relevant QoS management approaches into five main categories of QoS based controller design, Resource allocation-based approach, Queue scheduling and management-based approach, QoS-driven optimal routing, and Service Level Agreement (SLA) based quality management in SDN. We then compare the working of techniques in each category against the identified requirements for guaranteeing end-to-end QoS provisioning in SOA based SDN architecture and present directions for future research.</abstract>
	<keyword>Quality of Service (QoS);Software dEfined Networking (SDN);Service Oriented Architecture based SDN</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=27>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Tikiri—Towards a lightweight blockchain for IoT</paper_heading>
	<author>Eranga Bandara, Deepak Tosh, Peter Foytik, Sachin Shetty, ... Kasun De Zoysa</author>
	<abstract>Internet of Things (IoT) platforms have been deployed in several domains to enhance efficiency of business process and improve productivity. Most IoT platforms comprise of heterogeneous software and hardware components which can potentially introduce security and privacy challenges. Blockchain technology has been proposed as one of the solutions to realize IoT security by leveraging the (a) Immutable ledger, (b) Decentralized architecture and (c) Strong cryptography primitives. However, integrating blockchain platforms with IoT based applications presents several challenges due to lack of (a) acceptable performance on resource-constrained devices, (b) high transaction throughput, (c) keyword-based search and retrieve, (d) transaction back pressure operations, and (e) real-time response. In this paper, we propose a lightweight blockchain platform, “Tikiri”, for resource-constrained IoT devices. Tikiri uses Apache Kafka for the consensus and proposes new blockchain architecture to handle real-time transaction execution on the blockchain. Tikiri is characterized by functional programming and actor-based smart contract platform that realizes concurrent execution of transactions in the blockchain. Tikiri realizes a lightweight and scalable blockchain that can provides performance on the resource-constrained IoT devices.</abstract>
	<keyword>Blockchain;Smart contact;Edge computing;IoT;Big data</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=28>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Network embedding enhanced intelligent recommendation for online social networks</paper_heading>
	<author>Jianghao Li, Guo Yang</author>
	<abstract>With the fast development of Internet technology, more and more online social networks are changing our daily life. Whether or not an accurate recommendation can be provided for each user in the massive amount of information directly affects the user’s enthusiasm for receiving network services and the user experience effect, which in turn determines the user’s participation and loyalty to network applications. However, most previous methods only use a single network topology information and ignore other auxiliary information (such as user content information). Moreover, how to deal with large scale network is a challenging task. To tackle these challenges, we propose a topic-aware network embedding approach for providing intelligent recommendation services. Specifically, we first extract the network topology based on the constructed social network. Then, we extract the topic information based on the context released by the users with the help of topic model. Finally, a topic-aware network embedding framework is utilized for recommendation. Experimental results on two-widely used dataset demonstrate that our method can achieve the best performance.</abstract>
	<keyword>Online social network;Intelligent recommendation;Network embedding</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=29>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automatic stroke generation for style-oriented robotic Chinese calligraphy</paper_heading>
	<author>Gan Lin, Zhihua Guo, Fei Chao, Longzhi Yang, ... Changjing Shang</author>
	<abstract>Intelligent robots, as an important type of Cyber–Physical systems, have promising potential to take the central stage in the development of the next-generation of efficient smart systems. Robotic calligraphy is such an attempt, and the current research focuses on the control algorithms of the robotic arms, which usually suffers from significant human inputs and limited writing styles. This paper presents an autonomous robotic writing system for Chinese calligraphy empowered by the proposed automatic stroke matching and generation mechanisms. Thanks to these mechanisms, the robot is able to effectively learn to write any Chinese characters in a style that is sampled by a small amount of handwritten Chinese characters with a certain target writing style. This is achieved by firstly disassembling each given Chinese character into individual strokes using the proposed character disassemble method; then, the writing style of the dissembled strokes is learned by a stroke generation module, which is built upon a generative adversarial learning model. From this, the robot can apply the learned writing style to any Chinese character from a given database, by dissembling the character and then generating the stroke trajectories based on the learned writing style. The experiments confirm the effectiveness of the proposed system in learning writing a certain style of characters based on a small style dataset, as evidenced by the high similarity between the robotic writing results and the handwritten ones according to the Fréchet Inception Distance.</abstract>
	<keyword>Intelligent robots;Smart cyber–physical systems;Robotic calligraphy;Robotic motion planning;Deep learning</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=30>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A hypergraph based Kohonen map for detecting intrusions over cyber–physical systems traffic</paper_heading>
	<author>Sujeet S. Jagtap, Shankar Sriram V. S., Subramaniyaswamy V.</author>
	<abstract>Cyber–Physical System acts as a cornerstone in Industry 4.0 by integrating information-technology, electrical, and mechanical engineering under the same crown. This cybernetic–mechatronic augmentation expanded the attack vectors in critical infrastructure’s network, which gained the attraction of both cyber-offenders and cybersecurity researchers. Though the recent research works focus on developing proficient cybersecurity mechanisms, they often fail to address the major challenges such as handling the unseen zero-day exploits and detecting data irregularities that result in a poor attack detection rate. Hence to address the aforementioned challenges, this research article proposes an intelligent multi-level intrusion detection system to detect data-abnormalities in process-control network packets. The proposed approach involves the following phases: (i) Bloom-filter based payload level detection, (ii) partition-based Kohonen mapping for learning abnormal data patterns using a deep version of Kohonen neural network enhanced by principal component analysis and partitioning property of Hypergraph, and (iii) BLOSOM – a hybrid anomaly detection model. The impact of the proposed approach has been validated with the high-dimensional and heterogeneous benchmark datasets obtained from Mississippi State University (Gas-pipeline dataset) and Singapore University of Technology and Design (Secure WAter Treatment dataset). The proposed approach outscores the existing State-of-the-art approaches in terms of Precision, Recall, F-Score & Classification Accuracy and found to be robust, scalable & computationally attractive.</abstract>
	<keyword>Industrial control;Process control;SCADA systemsIntrusion detection;Communication system security;Kohonen feature maps</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=31>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evaluating verification awareness as a method for assessing adaptation risk</paper_heading>
	<author>Ian Riley, Sharmin Jahan, Allen Marshall, Charles Walter, Rose F. Gamble</author>
	<abstract>Self-integration requires a system to be self-aware and self-protecting of its functionality and communication processes to mitigate interference in accomplishing its goals. Incorporating self-protection into a framework for reasoning about compliance with critical requirements is a major challenge when the system’s operational environment may have uncertainties resulting in runtime changes. The reasoning should be over a range of impacts and tradeoffs in order for the system to immediately address an issue, even if only partially or imperfectly. Assuming that critical requirements can be formally specified and embedded as part of system self-awareness, runtime verification often involves extensive on-board resources and state explosion, with minimal explanation of results. Model-checking partially mitigates runtime verification issues by abstracting the system operations and architecture. However, validating the consistency of a model given a runtime change is generally performed external to the system and translated back to the operational environment, which can be inefficient. This paper focuses on codifying and embedding verification awareness into a system. Verification awareness is a type of self-awareness related to reasoning about compliance with critical properties at runtime when a system adaptation is needed. The premise is that an adaptation that interferes with a design-time proof process for requirement compliance increases the risk that the original proof process cannot be reused. The greater the risk to limiting proof process reuse, the higher the probability that the requirement would be violated by the adaptation. The application of Rice’s 1953 theorem to this domain indicates that determining whether a given adaptation inherently inhibits proof reuse is undecidable, suggesting the heuristic, comparative approach based on proof metadata that is part of our approach. To demonstrate our deployment of verification awareness, we predefine four adaptations that are all available to three distinct wearable simulations (hearables, stress, and insulin delivery). We capture metadata from applying automated theorem proving to wearable requirements and assess the risk among the four adaptations for limiting the proof process reuse for each of their requirements. The results show that the adaptations affect proof process reuse differently on each wearable. We evaluate our reasoning framework by embedding checkpoints for requirement compliance within the wearable code and log the execution trace of each adaptation. The logs confirm that the adaptation selected by each wearable with the lowest risk of inhibiting proof process reuse for its requirements also causes the least number of requirement failures in execution.</abstract>
	<keyword>Self-awareness;Verification awareness;Adaptation;Risk assessment</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=32>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Special issue on “self-improving self integration”</paper_heading>
	<author>Kirstie L. Bellman, Ada Diaconescu, Sven Tomforde</author>
	<abstract>“Self-improving system integration” (SISSY) is a research initiative that aims to master the ever-changing demands of system organisation in the presence of highly-dynamic autonomous subsystems, evolving architectures, and unpredictable open environments. In contrast to traditional system modelling, design and development – typically performed by engineers, offline – the SISSY vision promotes automated integration-related decisions, performed during run-time. This implies a change of responsibilities, from engineers to the system itself, which must now find ways to restructure and reassemble itself so as to achieve its objectives. In addition to self-integrating and self-improving internally, such systems must equally be able to cooperate or co-exist with other autonomous systems, which may dynamically join a shared environment. In this editorial, we briefly introduce the ‘self-improving system integration’ initiative and summarise the contents of this special issue.</abstract>
	<keyword>Self-integration;Self-improvement;Autonomous systems;Organic computing;System engineering;Editorial</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=33>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Emotion recognition by deeply learned multi-channel textual and EEG features</paper_heading>
	<author>Yishu Liu, Guifang Fu</author>
	<abstract>Human emotion recognition is a key technique in human–computer interaction. Traditional emotion recognition algorithms rely on external actions such as facial expression, which may fail to capture real human emotion since facial expression signals may be camouflaged. EEG signal is closely close to human emotion, which can directly reflect human emotion. In this paper, we propose to learn multi-channel features from the EEG signal for human emotion recognition, where the EEG signal is generated by sound signal stimulation. Specifically, we apply multi-channel EEG and textual feature fusion in time-domain to recognize different human emotions, where six statistical features in time-domain are fused to a feature vector for emotion classification. The textual feature extraction is based. And we conduct EEG&textual-based feature extraction from both time and frequency domain. Finally, we train SVM for human emotion recognition. Experimental on DEAP dataset show that compared with frequency domain feature-based emotion recognition algorithms, our proposed method improves recognition accuracy rate.</abstract>
	<keyword>Emotion recognition;EEG feature;Multi-channel feature;Voice signal</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=34>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Objects detection toward complicated high remote basketball sports by leveraging deep CNN architecture</paper_heading>
	<author>Long Liu</author>
	<abstract>The analysis of high-difficulty action recognition technology in basketball is mainly to identify and analyze the physical behavior of basketball players in the video to complete the technical action. The purpose of video recognition is to provide an important guarantee for improving the level of basketball training. The current target recognition technology has achieved some results. It shows that the application of target detection technology in basketball sports scene is of great significance and can improve the effect of sports training. However, traditional sports target recognition is limited by technology and injury, and the analysis of difficult sports skills is limited by the scene, dynamic background and technology, and cannot achieve the desired effect. This is not conducive to the improvement of athletes’ skills. Therefore, this article aims to develop a big data motion target detection system based on deep convolutional neural network for sports difficult motion image recognition. More specifically, we use the high discriminative power of the convolutional neural network to extract images to perform computational preprocessing for the recognition of each human motion image in the video stream. Then, the skeleton recognition algorithm based on LSTM is used to detect the key points of the human body, which is of great significance for modeling different movements. Finally, we developed an object detection system to reconstruct each movement. By selecting five groups of highly difficult actions that are likely to cause sports injuries to conduct experimental research, the results prove the effectiveness of the target detection system we proposed.</abstract>
	<keyword>Object detection;Sport action recognition;Image recognition;Basketball recognition</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=35>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Sports motional characteristics modeling by leveraging multi-modal image technique</paper_heading>
	<author>Yanxia Li, Ke Zhao</author>
	<abstract>Sport motional characteristic modeling is a hot research topic in human–computer interaction (HCI). Traditional sport motional characteristic based on single signal cannot achieve satisfactory performance. To solve this problem, we propose a multi-model feature fusion-based method for sport motional feature control. More specifically, we leverage Kinect sensor to acquire real-time video stream, where the main goal is to capture hand gesture as well as arm movements. HSV-based image segmentation is used for hand image patches extraction and recognition. To improve the effectiveness of sport motional characteristics control, we design audio-based HCI system to assist sport control. Our experiment is conducted on a quadruped robot platform with manipulator. Experimental results show the effectiveness of our proposed method.</abstract>
	<keyword>Multi-model feature fusion;Sport motion;Human–computer interaction;Kinect sensor</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=36>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Video quality evaluation toward complicated sport activities for clustering analysis</paper_heading>
	<author>Wei Yang, Jian Wang, Jinlong Shi</author>
	<abstract>Automatically clustering various sophisticated human activities (e.g., dancing, martial arts, and gymnastics) based on their quality scores is an indispensable technique in physical training, human–computer interaction, etc. Conventionally, many action recognition models are built upon the visual/semantic appearance of human body movements. Recently, due to the introduction of Microsoft Kinect, many skeleton-based human action understanding frameworks have been proposed. In this work, we propose a novel method to cluster the quality of complicated human actions towards contactless operative video reading system (COVRS). More specifically, we first extract the skeleton by leveraging the Kinect, which is subsequently fed into an aggregation deep neural network to extract the deep feature for each human action skeleton. In COVRS, the human hand gesture is an informative clue. Thus, we propose a ranking algorithm to extract the position of human five figures, based on which the deep hand gesture representation is hierarchically learned. Noticeably, it is observable that, the acoustic feature from many human activities also contributes to the quality assessment. We extract multiple acoustic features from the audio associated with each human activity video. Finally, based on the above human skeleton and hand gesture deep features, as well as the shallow acoustic features, we employ a probabilistic model to integrate them for clustering the various human activities using the quality of COVRS. Comprehensive experimental have demonstrated the effectiveness and efficiency of our method. Besides, empirical results have shown that our probabilistic quality model is highly extensible, where additionally visual/acoustic features can be encoded according to different applications.</abstract>
	<keyword>Quality assessment;Human activities;Deep learning;Kinect skeleton feature;Sport activity</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=37>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Inter/intra-category discriminative features for aerial image classification: A quality-aware selection model</paper_heading>
	<author>Yuanjin Xu, Ming Wei, M.M. Kamruzzaman</author>
	<abstract>Classification, recognition, and quality assessment of aerial images strongly depends on detecting and identifying their discriminative visual features. In practice, aerial images provide clues for various applications, including disaster prediction, automatic navigation, and military target detection. However, the detection of discriminative cues in aerial images is quite problematic since the aerial image quality is susceptible to luminance and noise, while aerial images have significantly different topological structures. We propose a novel method to explore quality-related and topological cues from aerial images for visual classification to mitigate these problems. We first decompose aerial images into several components, each being processed via the morphological filtering. Subsequently, we leverage the quality model to generate discriminative regions and topologies. Each aerial image is represented using a feature vector extracted from these regions. Afterward, we train a CNN-based visual classification model to predict aerial image categories. Experimental results have shown that our method can effectively predict aerial image categories, and the proposed algorithm is more robust than other state-of-the-art ones.</abstract>
	<keyword>Quality model;Visual classification;Discriminative feature;CNN;Feature selection</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=38>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>JOI: Joint placement of IoT analytics operators and pub/sub message brokers in fog-centric IoT platforms</paper_heading>
	<author>Daniel Happ, Suzan Bayhan, Vlado Handziski</author>
	<abstract>Internet of Things (IoT) systems are expected to generate a massive amount of data that needs to be processed. Given the large scale and geo-distributed nature of such systems, fog computing along with publish/subscribe (pub/sub) messaging has been proposed as possible solutions for coping with processing at scale. However, it is still unclear how practitioners can leverage the benefits of fog computing, e.g., how to optimally place data processing operators and pub/sub brokers. Moreover, current IoT systems typically rely on pub/sub brokers at the cloud, which might diminish the benefits offered by edge or fog processing as the communication between IoT operators has to be mediated by the brokers located in the cloud. To address this shortcoming, we propose to place the IoT application operators and the pub/sub brokers jointly on a network of nodes spanning from edge to the cloud considering various factors such as network topology or the locations of the IoT sensors and the consumers of the IoT applications. Different than the prior works, we specifically consider pub/sub brokers and their unique characteristics in the placement decision. First, we formulate the placement of operators and brokers jointly across edge, fog, and the cloud as a cost minimization problem. Next, we design two low-complexity heuristics. Our simulation results corroborate the argument that a placement in the cloud is usually a good option for IoT use cases, but also reveal the gap to the optimal solution in scenarios with heavier clustering of producers and consumers of sensor data. Studying the optimality gap shows that in such a setting heuristic solutions usually stay under a stretch factor of 2, with a worst case factor of 2.5 for a tabu-based solution and 2.85 for a greedy and a fixed placement in the cloud.</abstract>
	<keyword>Publish/subscribe;Operator placement;IoT;Cloud;Fog;Brokerage</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=39>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lightweight, Divide-and-Conquer privacy-preserving data aggregation in fog computing</paper_heading>
	<author>Kinza Sarwar, Sira Yongchareon, Jian Yu, Saeed ur Rehman</author>
	<abstract>With the increasing popularity of the Internet of Things’ (IoT) and fog computing paradigm, aggregating IoT data considering privacy concerns over fog networks can be seen as one of the biggest security challenges. Numerous schemes address this problem. However, most of the existing schemes and their associated methods are heavyweight, facing issues related to performance overhead. Furthermore, performing data aggregation at a single aggregator fog node causes an overly computational burden on the node, which results in high latency, degraded reliability and scalability leading to a single point of failure risks. To fill these gaps, this paper presents a lightweight, Divide-and-Conquer privacy-preserving data aggregation scheme in fog computing to improve data privacy, data processing, and storage capabilities. Particularly, we design a data division strategy based on the Level of Privacy (LoP) defined by data owners. The data division strategy not only effectively divides data according to LoP and distributes it among participating fog nodes for aggregation and storage processing, but also reduces computational and memory overhead in the processing simultaneously. Moreover, we perform a privacy analysis of our scheme and perform comprehensive experiments to compare it with other traditional schemes to evaluate performance efficiency. The results demonstrate that our scheme can efficiently achieve data privacy in fog computing and outperforms the other schemes in computational and memory costs.</abstract>
	<keyword>Data aggregation;Fog computing;IoT;Privacy</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=40>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DGSD: Distributed graph representation via graph statistical properties</paper_heading>
	<author>Anwar Said, Saeed-Ul Hassan, Suppawong Tuarob, Raheel Nawaz, Mudassir Shabbir</author>
	<abstract>Graph encoding methods have been proven exceptionally useful in many classification tasks — from molecule toxicity prediction to social network recommendations. However, most of the existing methods are designed to work in a centralized environment that requires the whole graph to be kept in memory. Moreover, scaling them on very large networks remains a challenge. In this work, we propose a distributed and permutation invariant graph embedding method denoted as Distributed Graph Statistical Distance (DGSD) that extracts graph representation on independently distributed machines. DGSD finds nodes’ local proximity by considering only nodes’ degree, common neighbors and direct connectivity that allows it to run in the distributed environment. On the other hand, the linear space complexity of DGSD makes it suitable for processing large graphs. We show the scalability of DGSD on sufficiently large random and real-world networks and evaluate its performance on various bioinformatics and social networks with the implementation in a distributed computing environment.</abstract>
	<keyword>Graph embedding;Distributed computing;Batch processing;Graph classification</keyword>
	<publication_year>2021_vol_119</publication_year>
</paper>
<paper no=41>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A highly efficient chain code for compression using an agent-based modeling simulation of territories in biological beavers</paper_heading>
	<author>Khaldoon Dhou, Christopher Cruzen</author>
	<abstract>The accelerated developments in technology led to a tremendous increase in the volumes of data to be transferred and exchanged between various network channels. These advancements create a huge demand for researchers to investigate new data compression techniques. Recent evidence from the literature shows that agent-based modeling is a promising direction to reduce the size of the data and change its original representation. In this article, the objective is to build an agent-based modeling simulation for chain coding and take advantage of it in data compression. Our agent-based model is inspired by the concept of defended territories of biological beavers. To this end, we use the pixel distribution in a bi-level image to construct a virtual environment of agents, add the beavers, and build territories around them. The main idea of defended beaver territories is to allow each beaver to maintain its area and protects it from intruders. To put it another way, defended territories allow beavers to work on different parts of an image while the algorithm tracks and records their movements, as well as manages disputes between them. Our research findings represent a further step towards employing the generated codes of movements in image processing operations other than coding and compression. Additionally, the experimental results showed that the current model was prosperous, and it could outperform many existing image compression techniques, including JBIG family methods. What’s more, paired-samples t-tests reveal that the mean differences between the outcomes of the current approach and each of the other standardized benchmarks we employed in comparison are statistically significant.</abstract>
	<keyword>Beavers;Compression;Beaver territorie;sBi-level image;Chain code;Agent-based model</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=42>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>It’s about Thyme: On the design and implementation of a time-aware reactive storage system for pervasive edge computing environments</paper_heading>
	<author>João A. Silva, Filipe Cerqueira, Hervé Paulino, João M. Lourenço, ... Nuno Preguiça</author>
	<abstract>Nowadays, smart mobile devices generate huge amounts of data in all sorts of gatherings. Much of that data has localized and ephemeral interest, but can be of great use if shared among co-located devices. However, mobile devices often experience poor connectivity, leading to availability issues if application storage and logic are fully delegated to a remote cloud infrastructure. In turn, the edge computing paradigm pushes computations and storage beyond the data center, closer to end-user devices where data is generated and consumed, enabling the execution of certain components of edge-enabled systems directly and cooperatively on edge devices. In this article, we address the challenge of supporting reliable and efficient data storage and dissemination among co-located wireless mobile devices without resorting to centralized services or network infrastructures. We propose Thyme, a novel time-aware reactive data storage system for pervasive edge computing environments, that exploits synergies between the storage substrate and the publish/subscribe paradigm. We present the design of Thyme and elaborate a three-fold evaluation, through an analytical study, and both simulation and real world experimentations, characterizing the scenarios best suited for its use. The evaluation shows that Thyme allows the notification and retrieval of relevant data with low overhead and latency, and also with low energy consumption, proving to be a practical solution in a variety of situations.</abstract>
	<keyword>Distributed storage;Publish/subscribe;Wireless networks;Mobile devices;Edge computing</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=43>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>FLAS: A combination of proactive and reactive auto-scaling architecture for distributed services</paper_heading>
	<author>Víctor Rampérez, Javier Soriano, David Lizcano, Juan A. Lara</author>
	<abstract>Cloud computing has established itself as the support for the vast majority of emerging technologies, mainly due to the characteristic of elasticity it offers. Auto-scalers are the systems that enable this elasticity by acquiring and releasing resources on demand to ensure an agreed service level. In this article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for distributed services that combines the advantages of proactive and reactive approaches according to the situation to decide the optimal scaling actions in every moment. The main novelties introduced by FLAS are (i) a predictive model of the high-level metrics trend which allows to anticipate changes in the relevant SLA parameters (e.g. performance metrics such as response time or throughput) and (ii) a reactive contingency system based on the estimation of high-level metrics from resource use metrics, reducing the necessary instrumentation (less invasive) and allowing it to be adapted agnostically to different applications. We provide a FLAS implementation for the use case of a content-based publish–subscribe middleware (E-SilboPS) that is the cornerstone of an event-driven architecture. To the best of our knowledge, this is the first auto-scaling system for content-based publish–subscribe distributed systems (although it is generic enough to fit any distributed service). Through an evaluation based on several test cases recreating not only the expected contexts of use, but also the worst possible scenarios (following the Boundary-Value Analysis or BVA test methodology), we have validated our approach and demonstrated the effectiveness of our solution by ensuring compliance with performance requirements over 99% of the time.</abstract>
	<keyword>Cloud;Elasticity;Automatic scaling;Distributed systems</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=44>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Proactive, uncertainty-driven queries management at the edge</paper_heading>
	<author>Kostas Kolomvatsos, Christos Anagnostopoulos</author>
	<abstract>esearch community has already revealed the challenges of data processing when performed at the Cloud that may affect the performance of any desired application. The main challenge is the increased latency observed when the data should ‘travel’ to the Cloud from the location they are collected and the waiting time for getting the final response. In an Internet of Things (IoT) scenario, this time could be critical for supporting real time applications. A solution to the discussed problem is the adoption of an Edge Computing (EC) approach where data can be processed close to their collection point. IoT devices could report data to a number of edge nodes that behave as distributed data repositories having the capability of processing them and producing analytics. Analytics should match the requirements of queries defined by end users or applications with the collected data and the characteristics of every edge node. However, when a query is defined, we should identify the appropriate edge node(s) to process it. In this paper, we propose an uncertainty management model to efficiently allocate every incoming query to the available edge nodes. Our scheme adopts the principles of the Fuzzy Logic (FL) theory and provides a decision making mechanism for the entity having the responsibility of the envisioned allocations. We combine the proposed uncertainty management scheme with a machine learning model based on a Support Vector Machine (SVM) to enhance the FL reasoning. Our aim is to manage all the hidden aspects of the problem combining two different technologies with different orientations. We also propose a methodology for the automated generation of the Footprint of Uncertainty (FoU) of membership functions involved in our interval Type-2 FL model. Our experimental evaluation aims at revealing the pros and cons of our mechanism presenting the results of extensive simulations adopting datasets found in the literature and a comparative analysis with other efforts in the domain.</abstract>
	<keyword>Edge computing;Internet of Things;Query allocation;Fuzzy Logic;Uncertainty management;Interval Type-2 fuzzy sets;Automated generation of fuzzy sets</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=45>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using deep belief network to demote web spam</paper_heading>
	<author>Xu Zhuang, Yan Zhu, Qiang Peng, Faisal Khurshid</author>
	<abstract>Many score propagation based Web Spam Demotion Algorithms (WSDAs) have been proposed in last decade. There are two major challenges those algorithms suffer from. First, the non-incremental property of score propagation based WSDAs restricted their applications in real world since Web changes rapidly and running algorithm on the entire Web graph is computation consuming. Second, the score propagation based WSDAs adopt only link structure of the web graph to demote Web spam, so that they are vulnerable to some other kind of spamming techniques, such as content spam. In this paper, we propose a preference-based learning to rank method to address the above-mentioned issues confronted by score propagation based WSDAs. Our proposal consists of two components, a preference function and an ordering algorithm. The preference function is modeled by Deep Belief Network (DBN), which can benefit from unlabeled data for better generalization. The proposed Incremental Probabilistic Ordering Algorithm (IPOA) uses the trained preference function to calculate top-ranking probabilities of Web pages, and then uses those probabilities for final ranking. Therefore, the complex object (i.e. Web page) ranking problem is reduced to real number ranking problem, which can be solved efficiently by classical sorting algorithm. We conduct experiments to compare our proposal with conventional score propagation based WSDAs as well as some popular preference based learning to rank algorithms on two public available datasets, WEBSPAM-UK2006 and WEBSPAM-UK2007. Our experimental results demonstrate the superiority of our proposed method. Specifically, compared with score propagation based WSDAs, we obtain 0.0074 absolute improvement (0.7% relative improvement) on WEBSPAM-UK2006 and 0.065 absolute improvement (7.3% relative improvement) on WEBSPAM-UK2007 in terms of spam demotion score.</abstract>
	<keyword>Web spam demotion;Deep belief network;Learning to rank</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=46>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Causal and Δ-causal broadcast in opportunistic networks</paper_heading>
	<author>Frédéric Guidec, Pascale Launay, Yves Mahéo</author>
	<abstract>Causal broadcast is a fundamental communication abstraction for many distributed applications. Several implementations of this abstraction have been proposed over the last decades for traditional networks, that is, networks that assume the existence of a continuous bi-directional end-to-end path between any pair of nodes. Opportunistic networks constitute a kind of networks in which this assumption cannot be made, though, so the implementation of causal broadcast in such networks must be addressed differently. This paper presents two algorithms based on causal barriers that can ensure the causally-ordered delivery of broadcast messages in an opportunistic network, considering both cases where the messages propagate in the network without or with a bounded lifetime. The latter case is especially interesting in networks that must run for a long time, or with a population of nodes that changes continuously.</abstract>
	<keyword>Opportunistic computing;Causal broadcast;causal broadcast</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=47>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the optimality of Concurrent Container Clusters Scheduling over heterogeneous smart environments</paper_heading>
	<author>A. Asensio, X. Masip-Bruin, J. Garcia, S. Sánchez</author>
	<abstract>Smart environments rely on the cloud for most computation activities; however, leveraging the availability of resources at the edge could complement the cloud capabilities to provide a resources continuum spectrum that takes advantage of the benefits of both technologies, cloud and edge computing. In this scenario, applications are usually decomposed for execution in sets of tasks, which are in turn encapsulated into virtual components such as containers. Containers are lightweight implementations of virtual machines, improving efficiency and portability in distributed applications. Traditional containers scheduling in cloud is a well-known problem, but when the environment is heterogeneous, as it is in the scope of edge computing and edge-cloud systems, the problem becomes more challenging. In this paper we present the Concurrent Container Clusters Scheduling problem (C3S) aimed at optimizing the problem of placing containers in clusters of heterogeneous nodes satisfying a set of resource requirements, quality of service limitations, and considering additional stringent constraints in terms of applications execution in isolation for security guaranteeing. The C3S problem has been formulated using Integer Linear Programming with the dual objective of minimizing the number of applications rejected while minimizing the number of nodes used for computation. We have evaluated the optimality of this approach, analyzed the performance in terms of solving time and, finally, created a heuristic approach to solve the problem in realistic high demanding scenarios.</abstract>
	<keyword>Edge computing;Edge-cloud systems;Optimal scheduling;Heterogeneous systems;Smart environments</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=48>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multi-modal aggression identification using Convolutional Neural Network and Binary Particle Swarm Optimization</paper_heading>
	<author>Kirti Kumari, Jyoti Prakash Singh, Yogesh K. Dwivedi, Nripendra P. Rana</author>
	<abstract>Aggressive posts containing symbolic and offencive images, inappropriate gestures along with provocative textual comments are growing exponentially in social media with the availability of inexpensive data services. These posts have numerous negative impacts on the reader and need an immediate technical solution to filter out aggressive comments. This paper presents a model based on a Convolutional Neural Network (CNN) and Binary Particle Swarm Optimization (BPSO) to classify the social media posts containing images with associated textual comments into non-aggressive, medium-aggressive and high-aggressive classes. A dataset containing symbolic images and the corresponding textual comments was created to validate the proposed model. The framework employs a pre-trained VGG-16 to extract the image features and a three-layered CNN to extract the textual features in parallel. The hybrid feature set obtained by concatenating the image and the text features were optimized using the BPSO algorithm to extract the more relevant features. The proposed model with optimized features and Random Forest classifier achieves a weighted F1-Score of 0.74, an improvement of around 3% over unoptimized features.</abstract>
	<keyword>Cyber-aggression;Cyberbullying;Multi-modal data;Convolutional Neural network;Binary Particle Swarm Optimization</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=49>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Holistic thermal-aware workload management and infrastructure control for heterogeneous data centers using machine learning</paper_heading>
	<author>SeyedMorteza MirhoseiniNejad, Ghada Badawy, Douglas G. Down</author>
	<abstract>Two key contributors to the energy expenditure in data centers are information technology (IT) equipment and cooling infrastructures. The standard practice of data centers lacks a tight correlation between these two entities, resulting in considerable power wastage. Considering the cooling cost of different locations inside a data center (cooling heterogeneity) and various cooling capabilities of servers (server heterogeneity) has significant potential for saving power, yet has not been studied thoroughly in the literature. There is a necessity for state-of-the-art approaches to integrate the control of IT and cooling units. Moreover, the literature still lacks an accurate and fast thermal model for temperature prediction inside a data center. In this paper, innovative approaches to quantify data center thermal heterogeneities are presented. Using data center thermal models the cost of providing cold air at the front of servers can be (indirectly) calculated, and the capability of servers to be cooled is formulated. Our approach assigns jobs to locations that are efficient to cool (from the perspectives of both servers and cooling units) and tunes cooling unit parameters. The method, called holistic data center infrastructure control (HDIC), has the potential to save a considerable amount of power by exploiting synergies between the workload scheduler and operational parameters of cooling units.</abstract>
	<keyword>Data center workload assignment;Cooling unit control;Thermal-aware scheduling;Neural network modeling;Data center model;Efficient cooling</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=50>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Early detection of cyberbullying on social media networks</paper_heading>
	<author>Manuel F. López-Vizcaíno, Francisco J. Nóvoa, Victor Carneiro, Fidel Cacheda</author>
	<abstract>Cyberbullying is an important issue for our society and has a major negative effect on the victims, that can be highly damaging due to the frequency and high propagation provided by Information Technologies. Therefore, the early detection of cyberbullying in social networks becomes crucial to mitigate the impact on the victims. In this article, we aim to explore different approaches that take into account the time in the detection of cyberbullying in social networks. We follow a supervised learning method with two different specific early detection models, named threshold and dual. The former follows a more simple approach, while the latter requires two machine learning models. To the best of our knowledge, this is the first attempt to investigate the early detection of cyberbullying. We propose two groups of features and two early detection methods, specifically designed for this problem. We conduct an extensive evaluation using a real world dataset, following a time-aware evaluation that penalizes late detections. Our results show how we can improve baseline detection models up to 42%.</abstract>
	<keyword>Cyberbullying;Social networks;Early detection;Machine learning</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=51>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programming framework and infrastructure for self-adaptation and optimized evolution method for microservice systems in cloud–edge environments</paper_heading>
	<author>Xiang He, Zhiying Tu, Xiaofei Xu, Zhongjie Wang</author>
	<abstract>Edge computing technologies facilitate the deployment of services on nearby edge servers with a large number of end users and their mobile devices to fulfill personalized demands. Owing to frequent changes in user mobility and demands, service systems deployed in an edge–cloud environment must continuously adapt to ensure that the quality of service (QoS) perceived by the end users is maintained at a stable and satisfactory level. As it is difficult for system operation engineers to manually deal with such frequent and large-scale evolution due to problems of cost and efficiency, self-adaptation of the system is essential. In this paper, we present a programming framework for microservices (EPF4M) and an infrastructure for self-adaptive microservice systems (EI4MS) for the cloud–edge environment based on microservice architecture. Our study follows a “monitoring–analyzing–planning–execution” control loop that empowers the service systems to redeploy the services according to changes in the QoS. A two-phase strategy is adopted to minimize the side effects of the loop on the performance of the service system. A prototype of this framework and infrastructure has been open-sourced and verified through experiments conducted in a real cloud–edge environment. The results demonstrate the usefulness and advantages of our approach.</abstract>
	<keyword>Self-adaptation;Microservice systems;Cloud–edge environment;Quality of Services (QoS);DevOps;Infrastructure</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=52>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scalable multi-channel dilated CNN–BiLSTM model with attention mechanism for Chinese textual sentiment analysis</paper_heading>
	<author>Chenquan Gan, Qingdong Feng, Zufan Zhang</author>
	<abstract>Due to the complex semantics of natural language, the multi-sentiment polarity of words, and the long-dependence of sentiments between words, the existing sentiment analysis methods (especially Chinese textual sentiment analysis) still face severe challenges. Aware of these issues, this paper proposes a scalable multi-channel dilated joint architecture of convolutional neural network and bidirectional long short-term memory (CNN–BiLSTM) model with an attention mechanism to analyze the sentiment tendency of Chinese texts. Through the multi-channel structure, this model can extract both the original context features and the multiscale high-level context features. Importantly, the number of the model channel can be optimally expanded according to the actual corpus. Furthermore, the attention mechanism including local attention and global attention is adopted to further distinguish the difference of features. The former is employed to weight the output features of each channel, and the latter is used to weight the fused features of all channels. Besides, an adaptive weighted loss function is designed to effectively avoid the imbalance of classes in training data. Finally, several experiments are performed to demonstrate the superior performance of the proposed model on two public datasets. Compared with word-level methods, the accuracy and Macro- are respectively increased by over 1.19% and 0.9% on NLPCC2017-ECGC corpus, the accuracy and  are respectively increased by more than 1.7% and 1.214% on ChnSentiCorp-Htl-unba-10000 corpus. Compared with char-level pre-training methods, the accuracy and Macro- also respectively achieve an improvement of over 3.416% and 4.324% on the NLPCC2017-ECGC corpus, the accuracy and  are respectively increased by more than 0.14% and 3% on the ChnSentiCorp-Htl-unba-10000 corpus.</abstract>
	<keyword>Chinese textual sentiment analysis;Dilated convolutional neural network;Bidirectional long short-term memory;Attention mechanism;Scalable multi-channel</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=53>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Security and Energy-aware Collaborative Task Offloading in D2D communication</paper_heading>
	<author>Zhongjin Li, Haiyang Hu, Hua Hu, Binbin Huang, ... Victor Chang</author>
	<abstract>Device-to-device (D2D) communication technique is used to establish direct links among mobile devices (MDs) to reduce communication delay and increase network capacity over the underlying wireless networks. Existing D2D schemes for task offloading focus on system throughput, energy consumption, and delay without considering data security. This paper proposes a Security and Energy-aware Collaborative Task Offloading for D2D communication (Sec2D). Specifically, we first build a novel security model, in terms of the number of CPU cores, CPU frequency, and data size, for measuring the security workload on heterogeneous MDs. Then, we formulate the collaborative task offloading problem that minimizes the time-average delay and energy consumption of MDs while ensuring data security. In order to meet this goal, the Lyapunov optimization framework is applied to implement online decision-making. Two solutions, greedy approach and optimal approach, with different time complexities, are proposed to deal with the generated mixed-integer linear programming (MILP) problem. The theoretical proofs demonstrate that Sec2D follows a  energy-delay tradeoff. Simulation results show that Sec2D can guarantee both data security and system stability in the collaborative D2D communication environment.</abstract>
	<keyword>D2D communication;Security modeling;Collaborative task offloading;Lyapunov optimization;Mixed-integer linear programming (MILP)</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=54>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance evaluation of Hyperledger Fabric-enabled framework for pervasive peer-to-peer energy trading in smart Cyber–Physical Systems</paper_heading>
	<author>Ankur Lohachab, Saurabh Garg, Byeong Ho Kang, Muhammad Bilal Amin</author>
	<abstract>The in-depth collaboration of Cyber–Physical Systems (CPSs) and smart grids constitute the novel paradigm of distributed energy trading, in which computation and process control are managed in an adaptive Peer-to-Peer (P2P) manner. To further strengthen this collaboration, Hyperledger Fabric (HF) can be prominently considered as a mean to implement next-generation secure and intelligent communication. However, implementing real-world applications on this platform may concern performance issues. For the constructive exploration of these issues, initially, we design a novel P2P energy trading framework for improving resource utilization and consequently addressing the impending electricity crisis challenge. Thenceforward, we evaluate the results based on the different system operational parameters for establishing a proof-of-concept. For determining performance bottlenecks and best-configuration, these results are investigated independently by using the Nectar Research Cloud, thereby sustaining scalability. The proposed evaluation approach will largely contribute to determining the system operational-level parameters of enterprise applications that will utilize the HF platform as their communication tool-support. In addition, a benchmark is presented based on the Hyperledger Caliper tool to facilitate application designers and developers in the form of selecting an appropriate implementation model across the two latest stable HF model versions. The illustrative CPS-enabled energy trading scenario corroborates the feasibility of the proposed framework to foster the development of HF-assisted smart P2P energy trading mechanisms.</abstract>
	<keyword>Blockchain;Hyperledger fabric;Cyber–physical systems;Smart grids;Energy trading;PHEV-to-PHEV;Performance benchmarking</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=55>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Do-Care: A dynamic ontology reasoning based healthcare monitoring system</paper_heading>
	<author>Hadda Ben Elhadj, Farag Sallabi, Amira Henaien, Lamia Chaari, ... Maryam Al Thawadi</author>
	<abstract>Healthcare remote monitoring applications dominate the market of new technologies due to their valuable aid to patients, families, and medical staff. They provide ubiquitous remote health services for patients with chronic diseases or specific conditions and can provide ubiquitous communication between patients and caregiver(s). This paper presents an ontology reasoning-based healthcare monitoring system called Do-Care. The proposed system supports the supervision and follow-up of outdoor and indoor patients suffering from chronic diseases. Collected data, from wearable1, nearable2 or usable3 devices forms the instances for entities from the proposed Do-Care ontology used by the reasoner when applying a set of SWRL4 rules to determinate the health situation of a patient as Normal, Abnormal or Wrong. The main contribution in this paper is a modular and dynamic ontology composed of FOAF5, SSN6/SOSA7 and ICNP8 ontologies with a scalable set of inference rules. The proposed rule based methodology is dynamic and adjustable to meet possible changes in the medication market, medical discoveries, and personal users’ profiles. The presented experimental results show the efficiency of the proposed DO-Care system.</abstract>
	<keyword>Dynamic ontology;Healthcare;IoT;Decision support system;Reasoning</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=56>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On unlinkability and denial of service attacks resilience of whistleblower platforms</paper_heading>
	<author>Silivanxay Phetsouvanh, Anwitaman Datta, Alwen Tiu</author>
	<abstract>This work explores how to enhance pseudonymous whistleblower submission systems, specifically by supporting protocol level unlinkability, while also making the system resilient against (distributed) denial of service attacks. To that end, we propose a blind signature based protocol which facilitates assignment of trust to anonymous posters in a manner which depends on the quality of prior posts, yet unlinkable to said posts or corresponding poster. This (multi-level) trust is leveraged to prioritize the posts, thus mitigating the effect that spam posts may have on the party reviewing the posts. We design and carry out simulations to explore the resilience of the whistleblower submission system against denial of service attacks while applying the proposed approach. Our experiments affirm that for a range of realistic scenarios the proposed approach provides reasonable mitigation.</abstract>
	<keyword>Anonymity;Unlinkability;Trust;Whistleblower platform;Denial of service</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=57>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Tracking sentiment towards news entities from Arabic news on social media</paper_heading>
	<author>Ali Al-Laith, Muhammad Shahbaz</author>
	<abstract>The tracking sentiment of the news entities over time provides important information to governments and enterprises during the decision-making process. Recently, it has attracted the attention of the research community as well due to its popularity in many applications including; tracking news about elections, e-commerce, and e-governance. However, most of the work is focused on English whereas limited contributions have been done for Arabic. Moreover, there are no annotated corpora in the Arabic news domain that can be used to perform the sentiment tracking task. In this research, we present an Arabic news corpus and its associated sentiment tracking system to monitor the sentiments towards news entities in the Arab world. Sentiment classification and Named Entity Recognition techniques are used to prepare the corpus for the tracking task. A sample dataset containing 7200 tweets was manually annotated to be used in building multiple classifiers and annotate more than 2.3M tweets using the semi-supervised technique. The results of sentiment classification by using different machine learning classifiers and internal testing set show that semi-automatically annotated dataset outperforms the manually annotated dataset by 23% and 16% on two-way and three-way classification respectively using F1-score. The tracking results illustrate that over time the sentiment tracking performs well at discovering the most popular entities, from social media and, tracking their shifts in different Arab regions. It can be used to detect the possible reasons for sentiment change over time and, to predict the future sentiment of the news entities.</abstract>
	<keyword>Sentiment analysis;Sentiment tracking;Social media;Arabic language;Arabic news;Machine learning;Semi-supervised annotation</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=58>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multi-robot path planning in wireless sensor networks based on jump mechanism PSO and safety gap obstacle avoidance</paper_heading>
	<author>Shasha Tian, Yuanxiang Li, Yilin Kang, Jiening Xia</author>
	<abstract>In order to meet the real-time and accurate requirements of multi-robot path planning in dynamic environment, this paper adopted wireless sensor network to locate robots and obstacles and used an improved artificial intelligent algorithm to plan path. In this paper, a jumping mechanism particle swarm optimization (JPSO) algorithm and a safety gap obstacle avoidance algorithm (SGOA) algorithm were proposed. Compared with canonical PSO algorithm, JPSO algorithm has three improvement strategies: Fitness value evaluation function, new learning sample and jumping strategy. The JPSO algorithm updates the particles with poor comprehensive quality by jumping and adjusts the inertia weight adaptively according to fitness value evaluation function. With the cooperation of new learning samples, the global searching ability and precision of the algorithm can be improved. SGOA algorithm is mainly aimed at the problem that robots with low priority are stuck in a long wait and cannot continue to walk when avoiding obstacles. By implementing the SGOA algorithm, a new collision-free safety path can be optimized for the robot with low priority. In order to verify JPSO and SGOA algorithm, a lot of experiments were done. JPSO algorithm was compared with two other improved PSO algorithms with 6 standard test functions. The path planning and obstacle avoidance experiments of six robots were realized using the JPSO and SGOA algorithm. The experimental results show that JPSO algorithm has higher accuracy and faster convergence speed than the other two improved PSO algorithms, and SGOA algorithm can solve the dynamic obstacle avoidance problem in the path planning of multiple robots well.</abstract>
	<keyword>Wireless sensor network;Multi-robot path planning;Jumping mechanism;Safety gap;Dynamic obstacle avoidance</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=59>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Effects of hidden layer sizing on CNN fine-tuning</paper_heading>
	<author>Stefano Marrone, Cristina Papa, Carlo Sansone</author>
	<abstract>Some applications have the property of being resilient, meaning that they are robust to noise (e.g. due to error) in the data. This characteristic is very useful in situations where an approximate computation allows to perform the task in less time or to deploy the algorithm on embedded hardware. Deep learning is one of the fields that can benefit from approximate computing to reduce the high number of involved parameters thanks to its impressive generalization ability. A common approach is to prune some neurons and perform an iterative re-training with the aim of both reducing the required memory and to speed-up the inference stage. In this work we propose to face CNN size reduction from a different perspective: instead of reducing the network weights or look for an approximated network very close to the Pareto frontier, we investigate whether it is possible to remove some neurons only from the fully connected layers before the network training without substantially affecting the network performance. As a case study, we will focus on “fine-tuning”, a branch of transfer learning that has shown its effectiveness especially in domains lacking effective expert-designed features. To further compact the network, we apply weight quantization to the convolutional kernels. Results show that it is possible to tailor some layers to reduce the network size, both in terms of the number of parameters to learn and required memory, without statistically affecting the performance and without the need for any additional training. Finally, we investigate to what extent the sizing operation affects the network robustness against adversarial perturbations, a set of approaches aimed at misleading deep neural networks.</abstract>
	<keyword>CNN;Adversarial perturbation;Fine-tuning;Approximate computing</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=60>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Building the Internet of Things platform for smart maternal healthcare services with wearable devices and cloud computing</paper_heading>
	<author>Xiaoqing Li, Yu Lu, Xianghua Fu, Yingjian Qi</author>
	<abstract>In recent years, the industry of wearable devices for pregnancies has been developing rapidly. The devices ranges from foetal monitors to multi-functional health examination instruments, which helped monitoring and management of maternal health indicators such as foetal heart rate, blood glucose, and blood pressure in home. Pregnant women and obstetricians are bounded together by wearable devices in an unprecedented way. With the universal use of the Internet of Things technology, the Smart Maternal construction started to raise people’s attention. This article proposes an Internet of Things platform for smart maternal healthcare services with wearable devices and cloud computing and the key technologies. We also investigate its applications, and monitoring and management modes in home for obstetrics departments in hospitals. The Smart Maternal platform, which is based on the Internet of Things and centres on pregnant women, is able to greatly mitigate the workload of medical staffs, increase the work efficiency, facilitate the pregnant women to go to the doctors and improve the quality of obstetrical treatment.</abstract>
	<keyword>Internet of Things;Wearable device;Pregnancy;M-healthcare;Cloud computing</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=61>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multi-channel, convolutional attention based neural model for automated diagnostic coding of unstructured patient discharge summaries</paper_heading>
	<author>Veena Mayya, Sowmya Kamath S., Gokul S. Krishnan, Tushaar Gangavarapu</author>
	<abstract>Effective coding of patient records in hospitals is an essential requirement for epidemiology, billing, and managing insurance claims. The prevalent practice of manual coding, carried out by trained medical coders, is error-prone and time-consuming. Mitigating this labor-intensive process by developing diagnostic coding systems built on patients’ Electronic Medical Records (EMRs) is vital. However, developing nations with low digitization rates have limited availability of structured EMRs, thereby necessitating a need for systems that leverage unstructured data sources. Despite the rich clinical information available in such unstructured data, modeling them is complex, owing to the variety and sparseness of diagnostic codes, complex structural and temporal nature of summaries, and prolific use of medical jargon. This work proposes a context-attentive network to facilitate automatic diagnostic code assignment as a multi-label classification problem. The proposed model facilitates information aggregation across a patient’s discharge summary via multi-channel, variable-sized convolutional filters to extract multi-granular snippets. The attention mechanism enables selecting vital segments in those snippets that map to the clinical codes. The model’s superior performance underscores its effectiveness compared to the state-of-the-art on the MIMIC-III database. Additionally, experimental validation using the CodiEsp dataset exhibited the model’s interpretability and explainability.</abstract>
	<keyword>Disease prediction;Explainability;Healthcare informatics;Interpretability;Predictive analytics;Unstructured text modeling</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=62>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Future generation of service-oriented computing systems</paper_heading>
	<author>Sami Yangui, Andrzej Goscinski, Khalil Drira, Zahir Tari, Djamal Benslimane</author>
	<abstract>Service-Oriented Computing (SOC) and SOC systems have been invented, studied, and developed in response to the problems generated by heterogeneity and poor latency, and a need for modularization and standardization. Wide application of SOC, practically in all areas of human endeavors, interconnectivity provided by wireless networks, huge volumes and diversity of data, increased frequency of security attacks, and reliability expectations have increased research, development, and deployment of SOC systems. These circumstances and requirements could only be dealt with by a future generation of SOC systems. This implies a need for fast and high-quality research and wide and efficient dissemination of achieved innovative results. Therefore, a Special Issue (SI) was proposed, and its relevant scope defined. This SI involves a set of high-quality and cross-community scientific papers from various disciplines (e.g., cloud computing, edge computing, business process management). The accepted papers focus on research leading toward future generation of service-oriented computing systems (e.g., dynamic quality of service management in fog computing, time-aware resources allocation in cloud computing, resilient composition in the Internet of Things) with emphasis on advanced results that solve open research problems and have significant impact on the field of service-oriented computing.</abstract>
	<keyword>Service computing;Cloud computing;Fog computing;Business Process Management</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=63>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A low-cost physical location discovery scheme for large-scale Internet of Things in smart city through joint use of vehicles and UAVs</paper_heading>
	<author>Haojun Teng, Mianxiong Dong, Yuxin Liu, Wang Tian, Xuxun Liu</author>
	<abstract>With the development of Information and Communication Technology (ICT), the construction of the smart city came into being. Compared with the traditional city, a smart city can reduce resource consumption, improve energy efficiency, reduce environmental pollution, reduce traffic congestion, reduce potential safety hazards, improve the quality of life of citizens, etc. In order to collect a large amount of data to provide accurate decision-making recommendations for the management of smart cities, a large-scale Internet of Things (IoT) system needs to be built as the basis. For most applications in smart cities, it is very important to obtain the physical location information of the data during the data collection. However, it is a challenging issue for most sensor devices in the IoT system, because sensor devices are hard to equip positioning equipment as limited by cost. To tackle this, a Low-Cost Physical Locations Discovery (LCPLD) Scheme is proposed in this paper. In LCPLD scheme, mobile vehicles and unmanned aerial vehicles (UAVs) are used for physical location discovery on the wireless sensor networks which are the important component of the IoT system in a smart city. In order to further reduce cost, we propose a task application mechanism to reduce the cost of vehicle broadcasting and the Adaptive UAV Flight Path Planning (AUPPP) algorithm to reduce UAV flight cost. In order to reduce localization error, the Large Error Rejection (LER) algorithm and the UAV Same Position Broadcast Repeat (USPBR) algorithm are proposed in this paper. After simulation experiments based on real vehicle driving data, the experimental results prove the effectiveness of the proposed scheme: Compared with the comparison scheme, the LCPLD scheme proposed has a cost reduction of 16.58% 19.88%, an average reduction of 78.80% in positioning error, and an average reduction of 99.88% in the variance of positioning error.</abstract>
	<keyword>Smart cityInternet of Things;Physical location discovery;Unmanned aerial vehicle;Mobile vehicles;Low cost</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=64>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Deflated reputation using multiplicative long short-term memory neural networks</paper_heading>
	<author>Yixuan Ma, Zhenji Zhang, Deming Li, Mincong Tang</author>
	<abstract>Current reputation systems are facing the inflation problem, which renders reputation systems to lose information and sometimes even cause misunderstandings. To address this problem, we propose a data-driven approach that combines natural language processing techniques with the conditional logit model for reputation deflation. We consider multiplicative long short-term memory neural networks (mLSTM) to predict sentiment scores from the feedback content. The mLSTM was pre-trained on 82.83 million unique reviews. We conduct experiments on one of the largest online labor marketplaces, Freelancer.com. We focus on comparing ratings and predicted sentiment scores in the online labor market. The results show that our proposed model can estimate deflated reputation information effectively. In addition, the estimated sentiment score is a quality disclosure signal, and has a better effect on the market outcome than the inflated reputation rating.</abstract>
	<keyword>Natural language processing (NLP);Sentiment analysis;Text mining;Reputation system</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=65>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Intelligent Dynamic Malware Detection using Machine Learning in IP Reputation for Forensics Data Analytics</paper_heading>
	<author>Nighat Usman, Saeeda Usman, Fazlullah Khan, Mian Ahmad Jan, ... Paul Watters</author>
	<abstract>In the near future, objects have to connect with each other which can result in gathering private sensitive data and cause various security threats and cyber crimes. To prevent cyber crimes, novel cyber security techniques are required that can identify malicious Internet Protocol (IP) addresses before communication. One of the best techniques is the IP reputation system used for profiling the behavior of security threats to the cyber–physical system. Existing reputation systems do not perform well due to their high management cost, false-positive rate, consumption time, and considering very few data sources for claiming IP address reputation. To overcome the aforementioned issues, we have proposed a novel hybrid approach based on Dynamic Malware Analysis, Cyber Threat Intelligence, Machine Learning (ML), and Data Forensics. Using the concept of big data forensics, IP reputation is predicted in its pre-acceptance stage and its associated zero-day attacks are categorized via behavioral analysis by applying the Decision Tree (DT) technique. The proposed approach highlights the big data forensic issues and computes severity, risk score along with assessing the confidence and lifespan simultaneously. The proposed system is evaluated in two ways; first, we compare the ML techniques to attain the best F-measure, precision and recall scores, and then we compare the entire reputation system with the existing reputation systems. Our proposed framework is not only cross checked with external sources but also able to reduce the security issues which were neglected by existing outdated reputation engines.</abstract>
	<keyword>Cyber threat;Cyber security;Big data;Severity;Confidence level;Time to live;Machine learning;Essential selected features;Risk score</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=66>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient and reliable forensics using intelligent edge computing</paper_heading>
	<author>Abdul Razaque, Moayad Aloqaily, Muder Almiani, Yaser Jararweh, Gautam Srivastava</author>
	<abstract>Due to the increasing awareness and use of cloud and edge computing, society and industries are beginning to understand the benefits they can provide. Cloud and Edge are the future of information management, and they have transformed the Internet into an innovative and interactive computing platform. The ultimate goal of edge/cloud computing is to reduce the use of computing resources in the network, as well as support information sharing and intercommunication efforts within the network. Secure edge computing methodologies are applied in both open and heterogeneous network systems to protect them from many potential security threats. However, these approaches only provide passive protection for normal edge computing operations, and fail to address the security measures of several applications, particularly forensics in industrial settings. Forensics applications running on edge computing must be capable of support taking legal action against invaders for malicious damage or information theft. This paper proposes an efficient and reliable forensics framework (ERFF) to address industrial intelligent edge computing critical for the industry 4.0 implementation plan. The proposed ERFF consists of a detective module and validation model, with the detective module responsible for detecting the interaction between the client terminal and the edge resource, which means the investigator is capable of gathering the evidence securely. The security-validation model integrated with ERFF is far safer than sharing common key-based cryptographic approaches. The proposed conceptual framework is tested with Live Digital Forensic Framework for a Cloud (LDF2C), and results are compared with other existing industrial frameworks that fulfill fundamental ISO/IEC 17025 accreditation requirements, including Legal Reliable Forensic Framework (LRFF), Source Identification Network Forensics Framework (SINFF) and Logging Framework for Cloud Computing Forensic (LFCCF)). These frameworks were designed to support the digital forensic requirements of industry and academia, and experimental results validate the effectiveness of the proposed framework from reliability and efficiency perspectives as well as realistic scenarios</abstract>
	<keyword>Intelligent edge computing;Reliability and efficiency;Cloud computing;Peer-to-peer edges;Security;Forensics framework</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=67>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DAD: A Distributed Anomaly Detection system using ensemble one-class statistical learning in edge networks</paper_heading>
	<author>Nour Moustafa, Marwa Keshk, Kim-Kwang Raymond Choo, Timothy Lynar, ... Monica Whitty</author>
	<abstract>There are various data management and security tools deployed at the cloud for storing and analyzing big data generated by the Internet of Things (IoT) and Industrial IoT (IIoT) systems. There is a recent trend to move such tools to edge networks (closer to the users and the IoT/IIoT systems) to address limitations, especially latency and security issues, in cloud-based solutions. However, protecting edge networks against zero-day attacks is challenging, due to the volume, variety and veracity of data collected from the large numbers of IoT devices in edge networks. In this paper, we propose a Distributed Anomaly Detection (DAD) system to discover zero-day attacks in edge networks. The proposed system uses Gaussian Mixture-based Correntropy, a novel ensemble one-class statistical learning model, which is designed to effectively monitor and recognize zero-day attacks in real-time from edge networks. We also design an IoT-edge-cloud architecture to illustrate the complexity of edge networks and how one can deploy the proposed system at network gateways. The proposed system is evaluated using both NSL-KDD and UNSW-NB15 datasets. The findings reveal that the proposed system achieves better performance, in terms of detection accuracy and processing time, compared with five anomaly detection techniques.</abstract>
	<keyword>Anomaly detection;Edge computing;Edge networks;One-class learning;Gaussian mixture model;Correntropy technique</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=68>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Generation of overlapping clusters constructing suitable graph for crime report analysis</paper_heading>
	<author>Ankur Das, Janmenjoy Nayak, Bighnaraj Naik, Uttam Ghosh</author>
	<abstract>Cybercrime is a kind of criminal activity generally committed by cybercriminals or hackers. Crime activities are growing explosively all over the world which motivates the law enforcement agencies for systematic analysis of crimes. In many cases, crime information is stored as online text reports in an unstructured way and one report describes several different criminal activities. Analysis of these crime reports for identifying patterns and trends in crime and devising solutions to crime detection and prevention strategies are very challenging tasks. In this paper, the crime reports are preprocessed and relations among named entity pairs are extracted to give the structured form to the reports. Each extracted relation is converted to an -dimensional real-valued vector based on the concept of Word2Vec model of Natural Language Processing. Then a novel agglomerative graph partitioning algorithm using various graph centrality measures is applied to partition the extracted relations. All the extracted relations of a report which are in a single partition are replaced by the representative of that partition and thus each report is described by a set of distinct types of relations. Next, a graph for the set of reports is constructed in such a way that nodes are corresponding to the tuple of relations that describes the reports, and an edge between a pair of nodes is drawn only if the corresponding pair of relations are of a similar type of two different reports. The constructed graph is a disconnected graph with each connected component is a clique. These cliques are easily identified in linear time of the number of edges in the graph and each clique provides a cluster of reports. As each report is described by a set of relations of different types, so obtained clusters are overlapping clusters. The degree of membership of a report in a cluster is also identified in the paper. The proposed method is experimented, and compared with some state-of-the-art partition-based and overlapping clustering algorithms to demonstrate its effectiveness in the domain of crime corpora.</abstract>
	<keyword>Cybercrime;Crime report analysis;Overlapping clustering;Graph centrality measures;Unsupervised learning;Natural Language Processing</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=69>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DIDDOS: An approach for detection and identification of Distributed Denial of Service (DDoS) cyberattacks using Gated Recurrent Units (GRU)</paper_heading>
	<author>Saif ur Rehman, Mubashir Khaliq, Syed Ibrahim Imtiaz, Aamir Rasool, ... Ali Kashif Bashir</author>
	<abstract>Distributed Denial of Service (DDoS) attacks can put the communication networks in instability by throwing malicious traffic and requests in bulk over the network. Computer networks form a complex chain of nodes resulting in a formation of vigorous structure. Thus, in this scenario, it becomes a challenging task to provide an efficient and secure environment for the user. Numerous approaches have been adopted in the past to detect and prevent DDoS attacks but lack in providing efficient and reliable attack detection. As a result, there is still notable room for improvement in providing security against DDoS attacks. In this paper, a novel high-efficient approach is proposed named DIDDOS to protect against real-world new type DDoS attacks using Gated Recurrent Unit (GRU) a type of Recurrent Neural Network (RNN). Different classification algorithms such as Gated Recurrent Units (GRU), Recurrent Neural Networks (RNN), Naive Bayes (NB), and Sequential Minimal Optimization (SMO) are utilized to detect and identify DDoS attacks. Performance evaluation metrics like accuracy, recall, f1-score, and precision are used to evaluate the efficiency of the machine and deep learning classifiers. Experimental results yield the highest accuracy of 99.69% for DDoS classification in case of reflection attacks and 99.94% for DDoS classification in case of exploitation attacks using GRU.</abstract>
	<keyword>Cyberattack;Cybersecurity;DDoS;IDS;Deep learning;GRU;Malware;Network;RNN;Traffic</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=70>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Panoramic visual perception and identification of architectural cityscape elements in a virtual-reality environment</paper_heading>
	<author>Ruo-Xi Zhang, Le-Min Zhang</author>
	<abstract>The rapid development of new digital technologies, such as those of virtual reality (VR), human–computer interaction, and artificial intelligence, has provided new opportunities for intelligent cityscape management and perception. This paper provides a new set of intelligent methods for integrating, refining, and managing architectural cityscape elements with VR-based visual perception technologies. We firstly analyze multidimensional subjective perception ratings as well as objective visual data obtained via synchronous human experience tracking in panoramic VR environments. Based on this analysis, we reveal the functions and weights of various cityscape elements, examine the deep cityscape formation mechanism, and hence put forward refined strategies for cityscape development. This work represents a novel approach for cityscape perception and deep understanding, and significantly promotes the application of cutting-edge digital technologies in smart city planning and governance.</abstract>
	<keyword>Virtual reality;Smart cityUrban renewal;Multidimensional perceptionIn-depth visual perception;People-oriented</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=71>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Image/video aesthetic quality management based on fusing 3D CNN and shallow features</paper_heading>
	<author>Yujia Jiang, Xin Liu</author>
	<abstract>Aiming at the problem that image/videos are the streaming data captured by different acquisition modality, each kind of which has its particular quality evaluation mode, this paper proposes a video quality evaluation (VQA) model based on 3D CNN correlated with image/video data. In order to achieve accurate VQA value, the model adopted a Bi-LSTM-based architecture to process video-related image/ text as auxiliary part to provide additional information for VQA architecture. Firstly, the auxiliary part constructs feature vector for each video-related text by the model originating from Bi-LSTM and Seq2Seq. Then, the feature vector was feed to a well-trained decoder to reconstruct the original input data. Then, the feature vector complied with the image/video data are inputted into end-to-end VQA modal based on the 3D CNN straightly. Experimental results on the image/video dataset from the Internet show that the proposed model reflects the subjective quality of medical image more comprehensively, and has better overall image/video VQA assessment performance than the other state-of-the-art non-reference methods.</abstract>
	<keyword>Image/video data;VQA3D CNN;Video-related text;Bi-LSTM</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=72>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Learning hierarchical face representation to enhance HCI among medical robots</paper_heading>
	<author>Dianmin Sun, Honghua Zhao, Tao Song, Aiqin Liu, ... Xin Zhao</author>
	<abstract>In this paper, we propose a hierarchical framework for face recognition by learning deep representation. In order to exploit key patches for face recognition, we separate the entire image into several patches including eyes, nose, and mouth. A binary facegrid is generated to indicate the accurate position of the key patches in face image. The patches are fed into the hierarchical framework to learn the deep representation of the image. We leverage the PCA and SVM method for face recognition. Our face representation can enhance many medical robot applications. Comprehensive experiments have demonstrated that our proposed method can effectively recognize real human faces from fake samples.</abstract>
	<keyword>Face recognition;Deep learning;Hierarchical neural network;Medical robot HCI</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=73>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Massive-scale carbon pollution control and biological fusion under big data context</paper_heading>
	<author>Yi Liu, Jie Xu, Weijie Yi</author>
	<abstract>In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs’ cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.</abstract>
	<keyword>Internet-scale network;Dense subgraph mining;Low carbon;Multiple features;Biologically-aware</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=74>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Massive-scale graph-clustering-based data management based on multi-metrics</paper_heading>
	<author>Su Hu</author>
	<abstract>With the explosive growth in the amount of data nowadays, we have entered the era of the “big data”. In this circumstance, it is becoming more and more significant to effectively and efficiently managing such massive-scale data. Traditionally, data are managed by only one single and unified metric, which might be too restrict due to the incapability to represent their social attributes (i.e., each sample and its socially-connected neighbors in the feature space). To tackle this problem, we propose a multi-metric framework for data similarity management, wherein the multiple metrics are derived through a dense subgraph mining algorithm. More specifically, for each sample, we first extract multi-channel features to characterize it, based on which multiple massive-scale affinity graphs can be constructed accordingly. Afterward, for affinity graph from each feature channel, we discover the densely distributed subgraphs on it. Thereby, each sample’s social network can be obtained and the metric can be defined accordingly. Finally, we can fuse the multiple metrics corresponding to social networks from different feature channels. In this manner, we obtain the so-called multi-metric similarity measure, which is socially-aware and optimally fuses multi-channel features. Comprehensive experimental results on six publicly available data sets have demonstrated the competitiveness of our proposed multi-metric in massive-scale data classification and retrieval.</abstract>
	<keyword>Data management;Similarity measure;Socially-aware;Distributed Metrics;Multiple metrics</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=75>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>AI-guided resource allocation and rescue decision system for medical applications</paper_heading>
	<author>Ye Yu, Zhongheng Zhang, Rongju Sun, Haiping Liu, ... Feng Yuan</author>
	<abstract>With the robust growth in the social environment, millions of families have vast medical plans nowadays. Each client requires a personalized medical rescue decision, making an intelligent recommendation system highly important. In practice, wealthy families tend to purchase tailored medical services, while others tend to seek medical services from local community hospitals. Noticeably, trillions-scale of medical service transactions occurs daily, making the development of a fully automatic intelligent recommendation system to match the suitable medical services to clients urgent. In this work, we propose a novel Internet-scale deep architecture that automatically recommends personalized and tailored medical rescue services based on the previously discovered clients with different financial tolerance. Specifically, given a massive number of client families, first, we create a set of features that represents the multiple medically related attributes of each family. These features capture the income, health status, type of diseases, occupation, and so on. The features are then combined into a descriptive feature by multi-view learning, which can dynamically assign the importance of each feature channels. Based on the combined feature, affinity graphs are created on a large scale to represent the relations between the client families. Also, an efficient dense subgraph mining is used to categorize the million-scale clients into 10 different medical groups. Finally, various customized medical services can be tailored for each client in a fully automatic and efficient manner. Comprehensive experiments on our collected data sets demonstrated the competitiveness of our proposed intelligent recommendation system. Moreover, the results of visualized dense subgraph mining showed that the client families with different wealth levels can be distinguished accurately.</abstract>
	<keyword>Medical services;Families clients;Information recommendation;Deep learning for clustering;Dense subgraph mining</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=76>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Graph-CAT: Graph Co-Attention Networks via local and global attribute augmentations</paper_heading>
	<author>Liang Yang, Weixun Li, Yuanfang Guo, Junhua Gu</author>
	<abstract>Graph neural networks have achieved tremendous success in semi-supervised node classification. In this paper, we firstly analyse the propagation strategies in two milestone methods, Graph Convolutional Network (GCN) and Graph Attention Network (GAT), to reveal their underlying philosophies. According to our analysis, the propagations in GAT can be interpreted as learnable and asymmetric local attribute augmentations, while that of GCN can be interpreted as fixed and symmetric local attribute smoothing. Unfortunately, the local attribute augmentations in GAT is not adequate in certain circumstances, because the nodes tend to possess similar attributes in local neighbourhoods. With a toy experiment, we manage to demonstrate the necessity to incorporate global information. Therefore, we propose a novel Graph Co-ATtention Network (Graph-CAT), which performs both the local and global attribute augmentations based on two different yet complementary attention schemes. Extensive experiments in both the transductive and inductive tasks demonstrate the superiority of our Graph-CAT compared to the state-of-the-art methods.</abstract>
	<keyword>Graph neural network;Attention mechanism;Attribute augmentation</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=77>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Influence maximization in social graphs based on community structure and node coverage gain</paper_heading>
	<author>Zhixiao Wang, Chengcheng Sun, Jingke Xi, Xiaocui Li</author>
	<abstract>Influence maximization is an optimization problem in the area of social graph analysis, which asks to choose a subset of  individuals to maximize the number of influenced nodes at the end of the diffusion process. As individuals within a community have frequent contact and are more likely to influence each other, community-based influence maximization has attracted considerable attentions. However, this kind of works ignores the role of overlapping nodes in community structure, resulting in performance degradation in seeds selection. In addition, many existing community-based algorithms identify the final seeds only from the selected important communities, or they need to leverage the weights between local spread and global spread of a node. It is difficult to set suitable scales for important communities or to determine the weights for different spread. In this paper, we propose a novel influence maximization approach based on overlapping community structure and node coverage gain. Firstly, social graphs are partitioned into different overlapping communities by the algorithm of node location analysis in topological potential field. Secondly, a node coverage gain sensitive centrality measure is put up to evaluate the influence of each node locally within its belonging communities, which avoids the problem of local spread and global spread. Finally, seed nodes are directly selected by combining the detected community structure with the pre-designed strategy, without important communities identification. The comprehensive experiments under both the Uniform Independent Cascade model and the Weighted Independent Cascade model demonstrate that our proposed approach can achieve competitive influence spread, outperforming state-of-the-art works. Furthermore, our proposed approach exhibits stable performance on graphs with different scales and various structural characteristics.</abstract>
	<keyword>Social graph;Influence maximization;Overlapping community;Topological potential field;Node coverage gain</keyword>
	<publication_year>2021_vol_118</publication_year>
</paper>
<paper no=78>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Chaotic improved PICEA-g-based multi-objective optimization for workflow scheduling in cloud environment</paper_heading>
	<author>Peyman Paknejad, Reihaneh Khorsand, Mohammadreza Ramezanpour</author>
	<abstract>Mapping of workflow tasks to computational resources in the cloud environment has engendered research interest in workflow scheduling. As workflow scheduling belongs to NP-complete problem, so building an optimum workflow scheduler with reasonable performance and computation speed is very challenging in the heterogeneous distributed environment of clouds. Many existing studies deal with cloud workflow scheduling as a single or bi-objective optimization problem without considering some important requirements of the users or the providers. Therefore, it is highly desirable to formulate scheduling of the workflow applications as a Multi-objective Optimization Problem (MOP) taking into account the requirements from the user and the service provider. For example, the cloud workflow scheduler might wish to consider user’s Quality of Service (QoS) objectives, such as makespan and cost, as well as provider’s objectives, such as energy efficiency over the Virtual Machines (VMs). In addition, early convergence in existing algorithms is a problem that increases the number of repetitions for reaching a global optimum. To overcome these drawbacks, in this paper, an enhanced multi-objective co-evolutionary algorithm, called ch-PICEA-g, is proposed as an effective heuristic algorithm, where the logistic and tent maps as two chaotic systems are applied in generating chaotic values to overcome the permute convergence in the initial population and the genetic operators. Also, an improved fitness function is applied to increase the performance of original PICEA-g. The functionality of the proposed algorithm is validated by extensive experiments. The obtained results indicate that this proposed algorithm outperforms its counterparts in terms of different performance metrics.</abstract>
	<keyword>Cloud computing;Workflow scheduling;Evolutionary multi-objective optimization;Energy-consumption;Chaotic systems</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=79>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SpecMiner: Heuristic-based mining of service behavioral models from interaction traces</paper_heading>
	<author>Muhammad Ashad Kabir, Jun Han, Md. Arafat Hossain, Steve Versteeg</author>
	<abstract>Software behavioral models have proven useful for emulating and testing software systems. Many techniques have been proposed to infer behavioral models of software systems from their interaction traces. The quality of the inferred/mined model is critical to their successful use. While generalization is necessary to deduce concise behavioral models, existing techniques of inferring models, in general, overgeneralize what behavior is valid. Imprecise models include many spurious behaviors, and thus compromise the effectiveness of their use. In this paper, we propose a novel approach, named SpecMiner, that increases the precision of the behavioral model inferred from interaction traces. The essence of our approach is a heuristic-based generalization and truthful minimization. The set of heuristics include patterns to match input traces and generalize them towards concise model representations. Furthermore, we adopt a truthful minimization technique to merge these generalized traces. The key insight of our approach is to infer a concise behavioral model without compromising its precision. We present an empirical evaluation of how our approach improves upon the state-of-the-art specification inference techniques. The results show that our approach mines model with 100% precision and recall with a limited computation overhead.</abstract>
	<keyword>Mining invocation sequences;Finite state automata;Service behavioral models;InferenceHeuristics;Interaction traces</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=80>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Time-constrained and network-aware containers scheduling in GPU era</paper_heading>
	<author>Leonardo Rosa Rodrigues, Guilherme Piêgas Koslovski, Marcelo Pasin, Maurício Aronne Pillon, ... Charles Christian Miers</author>
	<abstract>The recent advances on data center management and applications development are reflected by lightweight containers technology and critical Quality-of-Service (QoS) requirements. Tenants encapsulate applications in containers abstracting away details on the infrastructure, and entrust its management framework with the provisioning of network and time QOS requirements. In this paper, we addressed this NP-hard scheduling problem proposing a GPU Accelerated Containers Scheduler (GPUACS). We model the joint allocation of network and containers with QoS requirements as a graph embedding problem. GPUACS innovates by refactoring two Multicriteria Decision Makings (MCDMs) to GPU model, as well as by defining an efficient data structure to speed up the comparison of time-evolving QoS requirements. GPUACS follows a modular and configurable architecture, and the scheduling objective function can be adjusted by selecting the MCDM method and setting the appropriated weights to guide the comparisons. An experimental analysis demonstrated the sensitivity that GPU-tailored MCDM methods have to schedule container requests considering critical time, network, and processing criteria, as well as multiple queueing policies.</abstract>
	<keyword>Scheduling;Multitenant;Multicriteria;GPU;Network</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=81>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Future DNA computing device and accompanied tool stack: Towards high-throughput computation</paper_heading>
	<author>Shankai Yan, Ka-Chun Wong</author>
	<abstract>DNA Computing is still at its infant stage since its emergence. Multiple aspects of DNA computing have been studied but most of the research results have not been applied to the reality. It has been proved to exhibit high data storage density and support efficient random data access. It also shows the potential to provide alternative facilities for general computing. Especially, its natural double-helix structure enables it to be the best fit for high-throughput parallel computing. However, the underlying rationale of DNA computing is different from the existing electronic computing devices. Therefore, the popularity of DNA computing device is limited by its accessibility. We propose our designs for high-throughput DNA computing including DNA computing circuits, system architecture, and its compiler. We also demonstrate its feasibility using a simple example through simulation experiments. The objective of this framework is to bridge the gap between the existing computer developer community and the DNA computing biotechnology.</abstract>
	<keyword>DNA computing;DNA strand displacement;DNA compiler</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=82>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>I-Scheduler: Iterative scheduling for distributed stream processing systems</paper_heading>
	<author>Leila Eskandari, Jason Mair, Zhiyi Huang, David Eyers</author>
	<abstract>Task allocation in Data Stream Processing Systems (DSPSs) has a significant impact on performance metrics such as data processing latency and system throughput. An application processed by DSPSs can be represented as a Directed Acyclic Graph (DAG), where each vertex represents a task and the edges show the dataflow between the tasks. Task allocation can be defined as the assignment of the vertices in the DAG to the physical compute nodes such that the data movement between the nodes is minimised. Finding an optimal task placement for DSPSs is NP-hard. Thus, approximate scheduling approaches are required to improve the performance of DSPSs. In this paper, we propose a heuristic scheduling algorithm which reliably and efficiently finds highly communicating tasks by exploiting graph partitioning algorithms and a mathematical optimisation software package. We evaluate the communication cost of our method using three micro-benchmarks, showing that we can achieve results that are close to optimal. We further compare our scheduler with two popular existing schedulers, R-Storm and Aniello et al.’s ‘Online scheduler’ using two real-world applications. Our experimental results show that our proposed scheduler outperforms R-Storm, increasing throughput by up to 30%, and improves on the Online scheduler by 20%–86% as a result of finding a more efficient schedule.1</abstract>
	<keyword>Stream processing;Big data;Scheduling;Graph partitioning;Heterogeneous cluster</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=83>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Energy-aware scheduling in edge computing with a clustering method</paper_heading>
	<author>Yongsheng Hao, Jie Cao, Qi Wang, Jinglin Du</author>
	<abstract>With the development of Cloud and 5G technology, edge devices have been widely used in various areas. However, the limited battery energy and processing ability of edge devices hinder the usage scope of those devices. Prior studies have typically managed to immigrate virtual machines (VMs) or offload tasks to reduce energy consumption and shorten execution time. In our work, we consider devices that can obtain energy from a green energy source (such as wind energy, or solar energy). First, we use a clustering method to divide nodes into some clusters, each with some edge nodes to ensure the clusters have a minimum distance (defined by energy transferring attenuation ratios between nodes) between nodes in the cluster; the cluster center is a node with a VM. Then, based on the clustering method, a scheduling heuristic is proposed to transfer energy, immigrate VM, and allocate tasks. The simulation result shows that our proposed method reduces both the total energy consumption and the energy consumption from outside of the system (ECFO).</abstract>
	<keyword>Energy-aware;Cluster;Edge computing;Energy consumption</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=84>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An affective learning-based system for diagnosis and personalized management of diabetes mellitus</paper_heading>
	<author>Olatunji Mumini Omisore, Bolanle Adefowoke Ojokoh, Asegunoluwa Eunice Babalola, Tobore Igbe, ... Lei Wang</author>
	<abstract>Diabetes Mellitus is a major health problem with high global morbidity and mortality rates. While, conventional diagnosis methods are based on monitoring blood glucose levels, variables such as body mass index and diastolic blood pressure have been reported as having stable correlations with the incidence and prevalence of diabetes. Recently, machine learning approaches are developed for diagnosis of diabetes, but the existing models are mostly trained and validated on single dataset while the apt strategies needed for proper management of diabetes mellitus are omitted. In this study, we develop an affective learning-based system for diagnosis and therapy of diabetes mellitus. The integrated system consists of a multimodal adaptive neuro-fuzzy inference model designed for diagnosis of diabetes, and a knowledge-based diets recommender model designed for personalized management of diabetes. The diagnosis model was trained and validated with 87.5% and 13.5% of Pima Indians diabetes dataset, respectively; and re-validated with Schorling diabetes dataset; both of which are publicly available. Then, the model was applied retrospectively on a private dataset consisting of 14 female patients’ records obtained at Obafemi Awolowo University Teaching Hospital Complex, Ile-Ife, Nigeria. Also, the recommender model combines users’ diagnoses results with their eating formulae to determine users’ food-per-day consumption and generate weekly personalized food-plan from an expert-designed template. Evaluation results from the studies show that both models performed well. Specifically, the multimodal model attained training and validation accuracies of 83.8% and 79.2%, respectively for Pima dataset, and prediction accuracies of 72.9% and 94.3% for the Schorling and private dataset cases, respectively. In addition, the model shows the best performance when compared with individual baseline models, and ten existing machine learning methods used in related studies. Similarly, the proposed recommender model received the highest average score when compared with several existing diet recommender systems used for chronic diseases therapy. With these promising features, the proposed affective learning-based system could effectively reduce the morbidity and mortality rates of diabetes mellitus in the world.</abstract>
	<keyword>Diabetes mellitus;MANFISMedical diagnosis;Recommender system;Diet personalization;Affective systems</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=85>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An efficient identity-based proxy signcryption using lattice</paper_heading>
	<author>Hongfei Zhu, Ye Wang, Chonghua Wang, Xiaochun Cheng</author>
	<abstract>Signcryption combines the functionalities of digital signature and encryption in a single step, which promises to increase the efficiency and the confidentiality of on-line transactions. Identity-based proxy signcryption integrates the functions of identity-based cryptography, proxy signature and signcryption. However, identity-based proxy signcryption constructed on the discrete logarithm and the integer factoring problems cannot defeat quantum attack. Therefore, the paper proposes a lattice-based identity-based proxy signcryption since there are none efficient quantum algorithms to attack lattice-based cryptography. The constructed scheme has the properties of both signcryption and proxy signature, i.e., the constructed scheme has ind-cca2 security, suf-cma security, strong identifiability, strong undeniability and key dependence properties. Comparing with the known identity-based proxy signcryption, the proposed scheme needs smaller computation complexity, smaller secret key size and smaller signature tuple size. Besides, the proposed scheme does not depend on the public key infrastructure since it is an identity-based cryptography.</abstract>
	<keyword>On-line transaction security;Identity-based proxy signcryption;LatticeQuantum resilient security solution</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=86>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Online cloud resource prediction via scalable window waveform sampling on classified workloads</paper_heading>
	<author>Xiaogang Wang, Jian Cao, Dingyu Yang, Zhen Qin, Rajkumar Buyya</author>
	<abstract>Accurate prediction on the utilization of cloud resources is increasingly important for public cloud users, as it relates to the reasonable reservation of resources for minimizing the usage costs. However, the existing relevant approaches fail to predict the usage amount of cloud resources on the basis of the requested workloads of users’ applications, and the characteristics of changing workload data are rarely considered for the real-time prediction. To address these challenges, we propose an online cloud resource prediction model (OCRPM) to timely predict the proper resource usage amount. Firstly, all of the requested workloads are classified into three types of waveform trend patterns using the trend degree (TD). Next, a scalable window waveform sampling method (SWWS) on the classified patterns is devised to extend the suitable workload waveform interval window for supporting the subsequent high accurate prediction on the cloud resources. Finally, an optimal error gradient boosting regression (OEGBR) algorithm is given to train the data model and to predict the reasonable cloud resource usage amount in light of the requested workloads. The simulation results indicate that the proposed method can adjust the suitable workload waveform sampling window, and achieve higher prediction accuracy than the state-of-the-art relevant approaches and existing statistical learning models.</abstract>
	<keyword>Cloud computing;Resource prediction;Trend degree;Workload waveform sampling;Optimal error gradient boosting regression</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=87>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards user-centric, switching cost-aware fog node selection strategies</paper_heading>
	<author>Zeineb Rejiba, Xavier Masip-Bruin, Eva Marín-Tordera</author>
	<abstract>In order to address high latency issues that may arise when executing time-critical applications at the cloud side, the novel fog computing paradigm has emerged, thus enabling the execution of such applications within computation nodes present at the edge of the network. While executing such applications, a user may be moving in an area where a high number of heterogeneous fog nodes (FNs) co-exist. This makes the problem of selecting the most appropriate fog node to execute the user’s tasks challenging, especially since the set of visible FNs dynamically changes. Therefore, to deal with the uncertain and dynamic nature of such a fog computing environment, we model the FN selection problem using multi-armed bandits. However, standard solutions for the bandit problem are not tailored for scenarios with changing FN availabilities. In addition, since switching from one FN to the other causes a switching cost, such solutions lead to accumulating a high switching cost. Therefore, to address these issues, we first propose a block-based FN selection scheme, where switching among FNs is not allowed during a block of timeslots. We also propose a greedy approach, where FNs having a sufficiently good delay performance are selected in a greedy manner. Simulation results reveal that both approaches significantly improve the FN selection performance. In particular, we found that the block-based selection results in the lowest switching costs, whereas the greedy selection achieves the best overall performance.</abstract>
	<keyword>Fog computing;Edge computing;Mobility management;Multi-armed bandits;Switching cost</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=88>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Less-is-Better Protection (LBP) for memory errors in kNNs classifiers</paper_heading>
	<author>Shanshan Liu, Pedro Reviriego, Paolo Montuschi, Fabrizio Lombardi</author>
	<abstract>Classification is used in a wide range of applications to determine the class of a new element; for example, it can be used to determine whether an object is a pedestrian based on images captured by the safety sensors of a vehicle. Classifiers are commonly implemented using electronic components and thus, they are subject to errors in memories and combinational logic. In some cases, classifiers are used in safety critical applications and thus, they must operate reliably. Therefore, there is a need to protect classifiers against errors. The k Nearest Neighbors (kNNs) classifier is a simple, yet powerful algorithm that is widely used; its protection against errors in the neighbor computations has been recently studied. This paper considers the protection of kNNs classifiers against errors in the memory that stores the dataset used to select the neighbors. Initially, the effects of errors in the most common memory configurations (unprotected, Parity-Check protected and Single Error Correction-Double Error Detection (SEC-DED) protected) are assessed. The results show that surprisingly, for most datasets, it is better to leave the memory unprotected than to use error detection codes to discard the element affected by an error in terms of tolerance. This observation is then leveraged to develop Less-is-Better Protection (LBP), a technique that does not require any additional parity bits and achieves better error tolerance than Parity-Check for single bit errors (reducing the classification errors by 59% for the Iris dataset) and SEC-DED codes for double bit errors (reducing the classification errors by 42% for the Iris dataset).</abstract>
	<keyword>Classification;Memories;Error tolerancek nearest neighbors;Error control codes</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=89>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A double-blockchain solution for agricultural sampled data security in Internet of Things network</paper_heading>
	<author>Wei Ren, Xutao Wan, Pengcheng Gan</author>
	<abstract>The security of Internet of Things (IoT) devices’ sampled data in Agriculture is important as any malicious tampering or destroy may lead to wrong decision making and causes immeasurable losses. On the other hand, the emerging blockchain technology provides safer solutions in data protection and retracing. Yet when blockchain technology applied in agricultural data tracking and storage, the identity based searching is long and difficult. Also, public blockchain or consortium blockchain technology along cannot provide satisfactory solutions balancing between safety and high system performance. In this paper, we propose an Inter Planetary File System (IPFS) storage based double-blockchain solution for agricultural sampled data protection in IoT network. We use IPFS network to store the content of sampled data; the proposed system can retrieve the complete data segment by oracle mechanism. Then we design a consortium blockchain, Agricultural Sample Data Chain (ASDC), based on Ethereum technology and improve Merkle Patricia Trie (MPT) based account for every category of sampled data. Finally, after the data are stored in blocks in ASDC, block hash will be generated and upload on main chain of Ethereum to keep a public record in case malicious attacks occurs. Simulated results show that our proposed method has smaller time consumption than cloud storage and blockchain only storage; as well as our system is more robust than other two systems.</abstract>
	<keyword>Agricultural;Blockchain;Cross-chain;Data security;Merkle patricia trie</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=90>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach</paper_heading>
	<author>Ibrahim Alghamdi, Christos Anagnostopoulos, Dimitrios P. Pezaros</author>
	<abstract>An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.</abstract>
	<keyword>Mobile edge computing;Tasks offloading;Data quality;Optimal stopping theory;Sequential decision making</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=91>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using deep ensemble for influenza-like illness consultation rate prediction</paper_heading>
	<author>Endah Kristiani, Yuan-An Chen, Chao-Tung Yang, Chin-Yin Huang, ... Wei-Cheng Chan</author>
	<abstract>The cases of influenza have always been an important global issue. Not only does it affect people’s health, but also a crucial point in the policy implementation of government and health care units. If the trend of the epidemic can be predicted in advance, the public can be reminded to prevent it in advance to achieve control. This study aims to provide reliable forecasting services for changes in influenza-like epidemic trends. By collecting data on the emergency department visit rate of influenza-like illness, and linking the air pollution indicators and open data on temperature and humidity environmental factors, using the stacking method in the Ensemble Learning concept, first constructs deep learning models such like RNN, LSTM for the first stage training, and obtains the input data of the second stage model through cross-validation. This method combines the advantages of each model so that the model can have better performance. In addition, the outbreak calculation method published by the WHO is added to calculate the outbreak threshold of the influenza-like epidemic. This threshold will be used to determine whether the control status of the influenza-like illness is still in a reasonable control stage. The experiment confirmed that the model has a good prediction effect on the trend of the influenza-like epidemic, and the evaluation index MAPE value can get an excellent performance of 8% to 15%. Finally, historical data and future forecast data are integrated on the web page for visual presentation to show the actual status of regional air quality and influenza-like data, and predict whether there is a trend of the influenza-like outbreak in the region.</abstract>
	<keyword>Influenza-like illness;Ensemble learning;Deep learning;Outbreak Disease;LSTM</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=92>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Embedding reactive behavior into artifact-centric business process models</paper_heading>
	<author>Xavier Oriol, Giuseppe De Giacomo, Montserrat Estañol, Ernest Teniente</author>
	<abstract>In artifact-centric business process models it is usually assumed that the specification of the activities requires stating all the effects of the activity execution over the information base (i.e. over the artifacts it handles). In particular, these effects have to deal with integrity constraint enforcement to ensure a proper treatment of integrity constraints during activity execution. Manually specifying this treatment is a difficult, expensive and error-prone task, because of the inherent difficulty of getting rid of all the implication entailed by the constraints and also of the way to properly handle it. In this paper, we advocate for separating constraint handling from the specification of activities in such a way that only the effects of the activity over the artifacts have to be defined (without needing to care about the constraints). Then, we propose an approach to automatically generate an extension to the original business process model that allows identifying at run-time the additional updates that have to be applied to the information base to repair all constraint violations caused by the activity execution.</abstract>
	<keyword>Artifact-centric business process modeling;Constraint repair;BPMN;UML;OCL;Business process execution</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=93>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Handwriting dynamics assessment using deep neural network for early identification of Parkinson’s disease</paper_heading>
	<author>Iqra Kamran, Saeeda Naz, Imran Razzak, Muhammad Imran</author>
	<abstract>The etiology of Parkinson’s disease (PD) remains unclear. Symptoms usually appear after approximately 70% of dopamine-producing cells have stopped working normally. PD cannot be cured, but its symptoms can be managed to delay its progression. Evidence suggests that early diagnosis is important in establishing an effective pathway for management of symptoms. However, PD diagnosis is challenging, particularly in the early stages of the disease. In this paper, we present a method for early diagnosis of PD using patients’ handwriting samples. To improve performance, we combined multiple PD handwriting datasets and used deep transfer learning-based algorithms to overcome the challenge of high variability in the handwritten material. Our approach achieved excellent PD identification performance with 99.22% accuracy on illuminated task of combined HandPD, NewHandPD and Parkinson’s Drawing datasets, demonstrating the superiority of our approach over current state-of-the-art methods.</abstract>
	<keyword>Parkinson’s;Neurological disorder;Brain disorder;PD identification;Transfer learning</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=94>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Improving University Faculty Evaluations via multi-view Knowledge Graph</paper_heading>
	<author>Qika Lin, Yifan Zhu, Hao Lu, Kaize Shi, Zhendong Niu</author>
	<abstract>University faculties generate a large amount of heterogeneous data in e-learning environments that online systems and toolkits have made widely available in all aspects of teaching and scientific researching activities. How to use the data efficiently and scientifically for faculty evaluations has recently become an important issue in university performance systems. However, it is still a challenge to comprehensively assess faculty members using multi-source and multi-modal data due to the lack of uniform representations and evaluation processes. To this end, this paper proposes a novel University Faculty Evaluation System based on a multi-view Knowledge Graph (UFES-KG) that integrates heterogeneous faculty data. Relevant data, collected both on the Internet and through university-administered internal systems, includes faculty information such as scientific research papers, patents, funds, monographs, awards, professional activities and teaching performance. Furthermore, we construct entity representations through knowledge graph embedding methods to retain their semantic information. In addition, by integrating the academic development status of scholars in the previous three years as well as student evaluation data, this paper proposes an academic development factor (ADF) for making predictions about faculty academic development. The experimental results show that this factor is closely related to the features of the knowledge graph and student evaluations. In a certain case study, this factor is superior to the traditional h-index, g-index, and RG score. Intuitively and scientifically, this multi-view approach can improve evaluations of university faculties.</abstract>
	<keyword>University faculty evaluation;Knowledge graph;Academic development prediction;E-learning</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=95>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Neural machine translating from natural language to SPARQL</paper_heading>
	<author>Xiaoyu Yin, Dagmar Gromann, Sebastian Rudolph</author>
	<abstract>SPARQL is a highly powerful query language for an ever-growing number of resources and knowledge graphs represented in the Resource Description Framework (RDF) data format. Using it requires a certain familiarity with the entities in the domain to be queried as well as expertise in the language’s syntax and semantics, none of which average human web users can be assumed to possess. To overcome this limitation, automatically translating natural language questions to SPARQL queries has been a vibrant field of research. However, to this date, the vast success of deep learning methods has not yet been fully propagated to this research problem. This paper contributes to filling this gap by evaluating the utilization of eight different Neural Machine Translation (NMT) models for the task of translating from natural language to the structured query language SPARQL. While highlighting the importance of high-quantity and high-quality datasets, the results show a dominance of a Convolutional Neural Network (CNN)-based architecture with a Bilingual Evaluation Understudy (BLEU) score of up to 98 and accuracy of up to 94%.</abstract>
	<keyword>SPARQL;Neural Machine Translation;Natural language queries;Learning structured knowledge</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=96>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Self-improving system integration: Mastering continuous change</paper_heading>
	<author>Kirstie Bellman, Jean Botev, Ada Diaconescu, Lukas Esterle, ... Sven Tomforde</author>
	<abstract>The research initiative “self-improving system integration” (SISSY) was established with the goal to master the ever-changing demands of system organisation in the presence of autonomous subsystems, evolving architectures, and highly-dynamic open environments. It aims to move integration-related decisions from design-time to run-time, implying a further shift of expertise and responsibility from human engineers to autonomous systems. This introduces a qualitative shift from existing self-adaptive and self-organising systems, moving from self-adaptation based on predefined variation types, towards more open contexts involving novel autonomous subsystems, collaborative behaviours, and emerging goals. In this article, we revisit existing SISSY research efforts and establish a corresponding terminology focusing on how SISSY relates to the broad field of integration sciences. We then investigate SISSY-related research efforts and derive a taxonomy of SISSY technology. This is concluded by establishing a research road-map for developing operational self-improving self-integrating systems.</abstract>
	<keyword>Self-integration;Self-improvement;Autonomous systems;Taxonomy;Organic computing;System engineering</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=97>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Detecting impersonation attacks in cloud computing environments using a centric user profiling approach</paper_heading>
	<author>Hisham A. Kholidy</author>
	<abstract>Cloud computing has become the most needed technology for the IT industry. Impersonation attacks are among the most dangerous threats that Clouds face. In this paper, we present an approach to detect masquerade attacks in Clouds. The efficient detection of these attacks should correlate user behaviors in distinct environments and also should apply to several deployment models. We present and evaluate three approaches to detect Impersonation and masquerade attacks. The first approach analyzes sequences of correlated system calls from the VMs operating systems, while the second analyzes the NetFlow data from the network environment. The third approach integrates these two approaches by using a neural network that will produce better detections than any of the first two approaches. To simplify the testing and the evaluation of the three methods, the Cloud Intrusion Detection Dataset (CIDD) is used as a source for cloud audits data. The evaluation has considered alternative deployment models through our two intrusion detection frameworks, CIDS and CIDS-VIRT. The paper also shows that the proposed detection approaches are more accurate and outperform the SWAD-MMD, a recent masquerade detection framework that works in the cloud computing systems. Furthermore, the paper details our experimental results and evaluates the computational performance and the detection accuracy of these approaches.</abstract>
	<keyword>Impersonation detection;IDS;Audit correlation;Cloud computing;System calls;NetFlow;Feature extraction;Centric Profiling</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=98>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>End-to-end online performance data capture and analysis for scientific workflows</paper_heading>
	<author>George Papadimitriou, Cong Wang, Karan Vahi, Rafael Ferreira da Silva, ... Ian Foster</author>
	<abstract>With the increased prevalence of employing workflows for scientific computing and a push towards exascale computing, it has become paramount that we are able to analyze characteristics of scientific applications to better understand their impact on the underlying infrastructure and vice-versa. Such analysis can help drive the design, development, and optimization of these next generation systems and solutions. In this paper, we present the architecture, integrated with existing well-established and newly developed tools, to collect online performance statistics of workflow executions from various, heterogeneous sources and publish them in a distributed database (Elasticsearch). Using this architecture, we are able to correlate online workflow performance data, with data from the underlying infrastructure, and present them in a useful and intuitive way via an online dashboard. We have validated our approach by executing two classes of real-world workflows, both under normal and anomalous conditions. The first is an I/O-intensive genome analysis workflow; the second, a CPU- and memory-intensive material science workflow. Based on the data collected in Elasticsearch, we are able to demonstrate that we can correctly identify anomalies that we injected. The resulting end-to-end data collection of workflow performance data is an important resource of training data for automated machine learning analysis.</abstract>
	<keyword>Scientific workflows;Online performance monitoring;Extreme scale</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=99>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Entity-aware capsule network for multi-class classification of big data: A deep learning approach</paper_heading>
	<author>Amit Kumar Jaiswal, Prayag Tiwari, Sahil Garg, M. Shamim Hossain</author>
	<abstract>Named entity recognition (NER) is one of the most challenging natural language processing (NLP) tasks, as its performance is related to constantly evolving languages and dependency on expert (human) annotation. The diverse and dynamic content on the web significantly raises the need for a more generalized approach—one that is capable of correctly classifying terms in a corpus and feeding subsequent NLP tasks, such as machine translation, query expansion, and many other applications. Although extensively researched in recent times, the variety of public corpora available nowadays provides room for new and more accurate methods to tackle the NER problem. This paper presents a novel method that uses deep learning techniques based on the capsule network architecture for predicting entities in a corpus. This type of network groups neurons into so-called capsules to detect specific features of an object without reducing the original input unlike convolutional neural networks and their ‘max-pooling’ strategy. Our extensive evaluation on several benchmarked datasets demonstrates how competitive our method is in comparison with state-of-the-art techniques and how the usage of the proposed architecture may represent a significant benefit to further NLP tasks, especially in cases where experts are needed. Also, we explore NER using a theoretical framework that leverages big data for security. For the sake of reproducibility, we make the codebase open-source2 .</abstract>
	<keyword>Natural language processing;Named entity recognition;Capsule network</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
<paper no=100>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An ensemble machine learning approach through effective feature extraction to classify fake news</paper_heading>
	<author>Saqib Hakak, Mamoun Alazab, Suleman Khan, Thippa Reddy Gadekallu, ... Wazir Zada Khan</author>
	<abstract>There are numerous channels available such as social media, blogs, websites, etc., through which people can easily access the news. It is due to the availability of these platforms that the dissemination of fake news has become easier. Anyone using these platforms can create and share fake news content based on personal or professional motives. To address the issue of detecting fake news, numerous studies based on supervised and unsupervised learning methods have been proposed. However, all those studies do suffer from a certain limitation of poor accuracy. The reason for poor accuracy can be attributed due to several reasons such as the poor selection of features, inefficient tuning of parameters, imbalanced datasets, etc. In this article, we have proposed an ensemble classification model for detection of the fake news that has achieved a better accuracy compared to the state-of-the-art. The proposed model extracts important features from the fake news datasets, and the extracted features are then classified using the ensemble model comprising of three popular machine learning models namely, Decision Tree, Random Forest and Extra Tree Classifier. We achieved a training and testing accuracy of 99.8% and 44.15% respectively on the Liar dataset. For the ISOT dataset, we achieved the training and testing accuracy of 100%.</abstract>
	<keyword>Fake news detection;Ensemble machine learning;Feature extraction;Liar dataset;ISOT dataset</keyword>
	<publication_year>2021_vol_117</publication_year>
</paper>
